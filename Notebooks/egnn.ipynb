{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "U24J_ZPsHrtL"
      },
      "outputs": [],
      "source": [
        "from torch import nn\n",
        "import torch\n",
        "\n",
        "\n",
        "class E_GCL(nn.Module):\n",
        "    \"\"\"\n",
        "    E(n) Equivariant Convolutional Layer\n",
        "    re\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, input_nf, output_nf, hidden_nf, edges_in_d=0, act_fn=nn.SiLU(), residual=True, attention=False, normalize=False, coords_agg='mean', tanh=False):\n",
        "        super(E_GCL, self).__init__()\n",
        "        input_edge = input_nf * 2\n",
        "        self.residual = residual\n",
        "        self.attention = attention\n",
        "        self.normalize = normalize\n",
        "        self.coords_agg = coords_agg\n",
        "        self.tanh = tanh\n",
        "        self.epsilon = 1e-8\n",
        "        edge_coords_nf = 1\n",
        "\n",
        "        self.edge_mlp = nn.Sequential(\n",
        "            nn.Linear(input_edge + edge_coords_nf + edges_in_d, hidden_nf),\n",
        "            act_fn,\n",
        "            nn.Linear(hidden_nf, hidden_nf),\n",
        "            act_fn)\n",
        "\n",
        "        self.node_mlp = nn.Sequential(\n",
        "            nn.Linear(hidden_nf + input_nf, hidden_nf),\n",
        "            act_fn,\n",
        "            nn.Linear(hidden_nf, output_nf))\n",
        "\n",
        "        layer = nn.Linear(hidden_nf, 1, bias=False)\n",
        "        torch.nn.init.xavier_uniform_(layer.weight, gain=0.001)\n",
        "\n",
        "        coord_mlp = []\n",
        "        coord_mlp.append(nn.Linear(hidden_nf, hidden_nf))\n",
        "        coord_mlp.append(act_fn)\n",
        "        coord_mlp.append(layer)\n",
        "        if self.tanh:\n",
        "            coord_mlp.append(nn.Tanh())\n",
        "        self.coord_mlp = nn.Sequential(*coord_mlp)\n",
        "\n",
        "        if self.attention:\n",
        "            self.att_mlp = nn.Sequential(\n",
        "                nn.Linear(hidden_nf, 1),\n",
        "                nn.Sigmoid())\n",
        "\n",
        "    def edge_model(self, source, target, radial, edge_attr):\n",
        "        if edge_attr is None:  # Unused.\n",
        "            out = torch.cat([source, target, radial], dim=1)\n",
        "        else:\n",
        "            out = torch.cat([source, target, radial, edge_attr], dim=1)\n",
        "        out = self.edge_mlp(out)\n",
        "        if self.attention:\n",
        "            att_val = self.att_mlp(out)\n",
        "            out = out * att_val\n",
        "        return out\n",
        "\n",
        "    def node_model(self, x, edge_index, edge_attr, node_attr):\n",
        "        row, col = edge_index\n",
        "        agg = unsorted_segment_sum(edge_attr, row, num_segments=x.size(0))\n",
        "        if node_attr is not None:\n",
        "            agg = torch.cat([x, agg, node_attr], dim=1)\n",
        "        else:\n",
        "            agg = torch.cat([x, agg], dim=1)\n",
        "        out = self.node_mlp(agg)\n",
        "        if self.residual:\n",
        "            out = x + out\n",
        "        return out, agg\n",
        "\n",
        "    def coord_model(self, coord, edge_index, coord_diff, edge_feat):\n",
        "        row, col = edge_index\n",
        "        trans = coord_diff * self.coord_mlp(edge_feat)\n",
        "        if self.coords_agg == 'sum':\n",
        "            agg = unsorted_segment_sum(trans, row, num_segments=coord.size(0))\n",
        "        elif self.coords_agg == 'mean':\n",
        "            agg = unsorted_segment_mean(trans, row, num_segments=coord.size(0))\n",
        "        else:\n",
        "            raise Exception('Wrong coords_agg parameter' % self.coords_agg)\n",
        "        coord = coord + agg\n",
        "        return coord\n",
        "\n",
        "    def coord2radial(self, edge_index, coord):\n",
        "        row, col = edge_index\n",
        "        coord_diff = coord[row] - coord[col]\n",
        "        radial = torch.sum(coord_diff**2, 1).unsqueeze(1)\n",
        "\n",
        "        if self.normalize:\n",
        "            norm = torch.sqrt(radial).detach() + self.epsilon\n",
        "            coord_diff = coord_diff / norm\n",
        "\n",
        "        return radial, coord_diff\n",
        "\n",
        "    def forward(self, h, edge_index, coord, edge_attr=None, node_attr=None):\n",
        "        row, col = edge_index\n",
        "        radial, coord_diff = self.coord2radial(edge_index, coord)\n",
        "\n",
        "        edge_feat = self.edge_model(h[row], h[col], radial, edge_attr)\n",
        "        coord = self.coord_model(coord, edge_index, coord_diff, edge_feat)\n",
        "        h, agg = self.node_model(h, edge_index, edge_feat, node_attr)\n",
        "\n",
        "        return h, coord, edge_attr\n",
        "\n",
        "\n",
        "class EGNN(nn.Module):\n",
        "    def __init__(self, in_node_nf, hidden_nf, out_node_nf, in_edge_nf=0, device='cpu', act_fn=nn.SiLU(), n_layers=4, residual=True, attention=False, normalize=False, tanh=False):\n",
        "        '''\n",
        "\n",
        "        :param in_node_nf: Number of features for 'h' at the input\n",
        "        :param hidden_nf: Number of hidden features\n",
        "        :param out_node_nf: Number of features for 'h' at the output\n",
        "        :param in_edge_nf: Number of features for the edge features\n",
        "        :param device: Device (e.g. 'cpu', 'cuda:0',...)\n",
        "        :param act_fn: Non-linearity\n",
        "        :param n_layers: Number of layer for the EGNN\n",
        "        :param residual: Use residual connections, we recommend not changing this one\n",
        "        :param attention: Whether using attention or not\n",
        "        :param normalize: Normalizes the coordinates messages such that:\n",
        "                    instead of: x^{l+1}_i = x^{l}_i + Σ(x_i - x_j)phi_x(m_ij)\n",
        "                    we get:     x^{l+1}_i = x^{l}_i + Σ(x_i - x_j)phi_x(m_ij)/||x_i - x_j||\n",
        "                    We noticed it may help in the stability or generalization in some future works.\n",
        "                    We didn't use it in our paper.\n",
        "        :param tanh: Sets a tanh activation function at the output of phi_x(m_ij). I.e. it bounds the output of\n",
        "                        phi_x(m_ij) which definitely improves in stability but it may decrease in accuracy.\n",
        "                        We didn't use it in our paper.\n",
        "        '''\n",
        "\n",
        "        super(EGNN, self).__init__()\n",
        "        self.hidden_nf = hidden_nf\n",
        "        self.device = device\n",
        "        self.n_layers = n_layers\n",
        "        self.embedding_in = nn.Linear(in_node_nf, self.hidden_nf)\n",
        "        self.embedding_out = nn.Linear(self.hidden_nf, out_node_nf)\n",
        "        for i in range(0, n_layers):\n",
        "            self.add_module(\"gcl_%d\" % i, E_GCL(self.hidden_nf, self.hidden_nf, self.hidden_nf, edges_in_d=in_edge_nf,\n",
        "                                                act_fn=act_fn, residual=residual, attention=attention,\n",
        "                                                normalize=normalize, tanh=tanh))\n",
        "        self.to(self.device)\n",
        "\n",
        "    def forward(self, h, x, edges, edge_attr):\n",
        "        h = self.embedding_in(h)\n",
        "        for i in range(0, self.n_layers):\n",
        "            h, x, _ = self._modules[\"gcl_%d\" % i](h, edges, x, edge_attr=edge_attr)\n",
        "        h = self.embedding_out(h)\n",
        "        return h, x\n",
        "\n",
        "\n",
        "def unsorted_segment_sum(data, segment_ids, num_segments):\n",
        "    result_shape = (num_segments, data.size(1))\n",
        "    result = data.new_full(result_shape, 0)  # Init empty result tensor.\n",
        "    segment_ids = segment_ids.unsqueeze(-1).expand(-1, data.size(1))\n",
        "    result.scatter_add_(0, segment_ids, data)\n",
        "    return result\n",
        "\n",
        "\n",
        "def unsorted_segment_mean(data, segment_ids, num_segments):\n",
        "    result_shape = (num_segments, data.size(1))\n",
        "    segment_ids = segment_ids.unsqueeze(-1).expand(-1, data.size(1))\n",
        "    result = data.new_full(result_shape, 0)  # Init empty result tensor.\n",
        "    count = data.new_full(result_shape, 0)\n",
        "    result.scatter_add_(0, segment_ids, data)\n",
        "    count.scatter_add_(0, segment_ids, torch.ones_like(data))\n",
        "    return result / count.clamp(min=1)\n",
        "\n",
        "\n",
        "def get_edges(n_nodes):\n",
        "    rows, cols = [], []\n",
        "    for i in range(n_nodes):\n",
        "        for j in range(n_nodes):\n",
        "            if i != j:\n",
        "                rows.append(i)\n",
        "                cols.append(j)\n",
        "\n",
        "    edges = [rows, cols]\n",
        "    return edges\n",
        "\n",
        "\n",
        "def get_edges_batch(n_nodes, batch_size):\n",
        "    edges = get_edges(n_nodes)\n",
        "    edge_attr = torch.ones(len(edges[0]) * batch_size, 1)\n",
        "    edges = [torch.LongTensor(edges[0]), torch.LongTensor(edges[1])]\n",
        "    if batch_size == 1:\n",
        "        return edges, edge_attr\n",
        "    elif batch_size > 1:\n",
        "        rows, cols = [], []\n",
        "        for i in range(batch_size):\n",
        "            rows.append(edges[0] + n_nodes * i)\n",
        "            cols.append(edges[1] + n_nodes * i)\n",
        "        edges = [torch.cat(rows), torch.cat(cols)]\n",
        "    return edges, edge_attr"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "n_nodes = 31\n",
        "n_edges = 66\n",
        "\n",
        "h = torch.rand((n_nodes, 64))\n",
        "edge_index = torch.randint(0, n_nodes, (2, n_edges))\n",
        "edge_attr = torch.rand((n_edges, 4))\n",
        "edges = [edge_index[0], edge_index[1]]\n",
        "x = torch.rand((n_nodes, 3))\n",
        "pe = torch.rand((n_nodes, 20))\n",
        "batch = torch.zeros(n_nodes, dtype=torch.long)\n",
        "print(h.shape)\n",
        "print(edge_index.shape)\n",
        "print(edge_attr.shape)\n",
        "print(x.shape)\n",
        "print(pe.shape)\n",
        "print(batch.shape)\n",
        "print(edges[0].shape)\n",
        "print(edges[1].shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "72h0N03sRF1W",
        "outputId": "d03d650a-aa74-4459-be72-cb56aae213f3"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([31, 64])\n",
            "torch.Size([2, 66])\n",
            "torch.Size([66, 4])\n",
            "torch.Size([31, 3])\n",
            "torch.Size([31, 20])\n",
            "torch.Size([31])\n",
            "torch.Size([66])\n",
            "torch.Size([66])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "in_node_nf = 64      # Number of input node features\n",
        "hidden_nf = 32        # Number of hidden features\n",
        "out_node_nf = 64       # Number of output node features\n",
        "in_edge_nf = 4        # Number of edge features\n",
        "device = 'cuda:0' if torch.cuda.is_available() else 'cpu'  # Use GPU if available\n",
        "n_layers = 4          # Number of EGNN layers\n",
        "residual = True       # Enable residual connections\n",
        "attention = False     # Disable attention (default)\n",
        "normalize = False     # Do not normalize coordinates (default)\n",
        "tanh = False          # Do not use tanh activation on phi_x (default)\n",
        "\n",
        "# Initialize the model\n",
        "model = EGNN(\n",
        "    in_node_nf=in_node_nf,\n",
        "    hidden_nf=hidden_nf,\n",
        "    out_node_nf=out_node_nf,\n",
        "    in_edge_nf=in_edge_nf,\n",
        "    device=device,\n",
        "    act_fn=nn.SiLU(),   # Activation function\n",
        "    n_layers=n_layers,\n",
        "    residual=residual,\n",
        "    attention=attention,\n",
        "    normalize=normalize,\n",
        "    tanh=tanh\n",
        ")\n",
        "\n",
        "print(model)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "U986MjLSV6D3",
        "outputId": "e72c5ba3-0bce-4c67-fd32-0f5c86030b3c"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "EGNN(\n",
            "  (embedding_in): Linear(in_features=64, out_features=32, bias=True)\n",
            "  (embedding_out): Linear(in_features=32, out_features=64, bias=True)\n",
            "  (gcl_0): E_GCL(\n",
            "    (edge_mlp): Sequential(\n",
            "      (0): Linear(in_features=69, out_features=32, bias=True)\n",
            "      (1): SiLU()\n",
            "      (2): Linear(in_features=32, out_features=32, bias=True)\n",
            "      (3): SiLU()\n",
            "    )\n",
            "    (node_mlp): Sequential(\n",
            "      (0): Linear(in_features=64, out_features=32, bias=True)\n",
            "      (1): SiLU()\n",
            "      (2): Linear(in_features=32, out_features=32, bias=True)\n",
            "    )\n",
            "    (coord_mlp): Sequential(\n",
            "      (0): Linear(in_features=32, out_features=32, bias=True)\n",
            "      (1): SiLU()\n",
            "      (2): Linear(in_features=32, out_features=1, bias=False)\n",
            "    )\n",
            "  )\n",
            "  (gcl_1): E_GCL(\n",
            "    (edge_mlp): Sequential(\n",
            "      (0): Linear(in_features=69, out_features=32, bias=True)\n",
            "      (1): SiLU()\n",
            "      (2): Linear(in_features=32, out_features=32, bias=True)\n",
            "      (3): SiLU()\n",
            "    )\n",
            "    (node_mlp): Sequential(\n",
            "      (0): Linear(in_features=64, out_features=32, bias=True)\n",
            "      (1): SiLU()\n",
            "      (2): Linear(in_features=32, out_features=32, bias=True)\n",
            "    )\n",
            "    (coord_mlp): Sequential(\n",
            "      (0): Linear(in_features=32, out_features=32, bias=True)\n",
            "      (1): SiLU()\n",
            "      (2): Linear(in_features=32, out_features=1, bias=False)\n",
            "    )\n",
            "  )\n",
            "  (gcl_2): E_GCL(\n",
            "    (edge_mlp): Sequential(\n",
            "      (0): Linear(in_features=69, out_features=32, bias=True)\n",
            "      (1): SiLU()\n",
            "      (2): Linear(in_features=32, out_features=32, bias=True)\n",
            "      (3): SiLU()\n",
            "    )\n",
            "    (node_mlp): Sequential(\n",
            "      (0): Linear(in_features=64, out_features=32, bias=True)\n",
            "      (1): SiLU()\n",
            "      (2): Linear(in_features=32, out_features=32, bias=True)\n",
            "    )\n",
            "    (coord_mlp): Sequential(\n",
            "      (0): Linear(in_features=32, out_features=32, bias=True)\n",
            "      (1): SiLU()\n",
            "      (2): Linear(in_features=32, out_features=1, bias=False)\n",
            "    )\n",
            "  )\n",
            "  (gcl_3): E_GCL(\n",
            "    (edge_mlp): Sequential(\n",
            "      (0): Linear(in_features=69, out_features=32, bias=True)\n",
            "      (1): SiLU()\n",
            "      (2): Linear(in_features=32, out_features=32, bias=True)\n",
            "      (3): SiLU()\n",
            "    )\n",
            "    (node_mlp): Sequential(\n",
            "      (0): Linear(in_features=64, out_features=32, bias=True)\n",
            "      (1): SiLU()\n",
            "      (2): Linear(in_features=32, out_features=32, bias=True)\n",
            "    )\n",
            "    (coord_mlp): Sequential(\n",
            "      (0): Linear(in_features=32, out_features=32, bias=True)\n",
            "      (1): SiLU()\n",
            "      (2): Linear(in_features=32, out_features=1, bias=False)\n",
            "    )\n",
            "  )\n",
            ")\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "h, x = model(h, x, edges, edge_attr)\n",
        "print(h.shape, x.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lmHb6fLAWf_L",
        "outputId": "fd012251-4564-46bf-9cbb-e390e2c85c6d"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([31, 64]) torch.Size([31, 3])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def get_edges(n_nodes):\n",
        "    rows, cols = [], []\n",
        "    for i in range(n_nodes):\n",
        "        for j in range(n_nodes):\n",
        "            if i != j:\n",
        "                rows.append(i)\n",
        "                cols.append(j)\n",
        "    edges = [rows, cols]\n",
        "    return edges"
      ],
      "metadata": {
        "id": "mL9CKdIFTNU-"
      },
      "execution_count": 3,
      "outputs": []
    }
  ]
}