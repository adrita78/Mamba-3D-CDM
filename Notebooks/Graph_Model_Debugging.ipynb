{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3bV7v3HtfW73",
        "outputId": "be23e05f-59ec-4184-86e4-86cf7102a2f5"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'egnn'...\n",
            "remote: Enumerating objects: 92, done.\u001b[K\n",
            "remote: Counting objects: 100% (26/26), done.\u001b[K\n",
            "remote: Compressing objects: 100% (22/22), done.\u001b[K\n",
            "remote: Total 92 (delta 10), reused 4 (delta 4), pack-reused 66 (from 1)\u001b[K\n",
            "Receiving objects: 100% (92/92), 297.25 KiB | 5.04 MiB/s, done.\n",
            "Resolving deltas: 100% (17/17), done.\n"
          ]
        }
      ],
      "source": [
        "!git clone https://github.com/vgsatorras/egnn"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Lef2pzDc9MIY",
        "outputId": "19f63a76-f4d4-4278-b29b-2095ed733b41"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/egnn/models/egnn_clean\n"
          ]
        }
      ],
      "source": [
        "%cd /content/egnn/models/egnn_clean"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "1V3JHA8wBmYs"
      },
      "outputs": [],
      "source": [
        "!python egnn_clean.py"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "r6VbWOZ_6tVj"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import egnn_clean as eg"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(\"Device:\", device)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "W6yfHKH0YQv2",
        "outputId": "b8baea61-978c-42a0-c0b4-b91d8c71059a"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Device: cuda\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "PKLL9tITQ3vF"
      },
      "outputs": [],
      "source": [
        "def get_edges(n_nodes):\n",
        "    rows, cols = [], []\n",
        "    for i in range(n_nodes):\n",
        "        for j in range(n_nodes):\n",
        "            if i != j:\n",
        "                rows.append(i)\n",
        "                cols.append(j)\n",
        "    edges = [rows, cols]\n",
        "    return edges\n",
        "\n",
        "def get_edges_batch(n_nodes, batch_size):\n",
        "    edges = get_edges(n_nodes)\n",
        "    edge_attr = torch.ones(len(edges[0]) * batch_size, 1)\n",
        "    edges = [torch.LongTensor(edges[0]), torch.LongTensor(edges[1])]\n",
        "    if batch_size == 1:\n",
        "        return edges, edge_attr\n",
        "    elif batch_size > 1:\n",
        "        rows, cols = [], []\n",
        "        for i in range(batch_size):\n",
        "            rows.append(edges[0] + n_nodes * i)\n",
        "            cols.append(edges[1] + n_nodes * i)\n",
        "        edges = [torch.cat(rows), torch.cat(cols)]\n",
        "    return edges, edge_attr\n",
        "\n",
        "# Convert to edge_index\n",
        "def get_edge_index_batch(n_nodes, batch_size):\n",
        "    edges, edge_attr = get_edges_batch(n_nodes, batch_size)\n",
        "    edge_index = torch.stack(edges, dim=0)  # Stack rows and cols\n",
        "    return edge_index\n",
        "\n",
        "# Example usage\n",
        "#n_nodes = 4\n",
        "#batch_size = 8\n",
        "#edges, edge_attr = get_edges_batch(n_nodes, batch_size)\n",
        "#print(edge_attr.shape)\n",
        "#edge_index = get_edge_index_batch(n_nodes, batch_size)\n",
        "#print(edge_index.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "v2TovT3Oyl2m",
        "outputId": "bf23c61a-8d77-4d41-9ed4-39cc9b1eb256"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in links: https://data.pyg.org/whl/torch-2.5.1+cu121.html\n",
            "Collecting torch-sparse\n",
            "  Downloading https://data.pyg.org/whl/torch-2.5.0%2Bcu121/torch_sparse-0.6.18%2Bpt25cu121-cp310-cp310-linux_x86_64.whl (5.1 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.1/5.1 MB\u001b[0m \u001b[31m24.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: scipy in /usr/local/lib/python3.10/dist-packages (from torch-sparse) (1.13.1)\n",
            "Requirement already satisfied: numpy<2.3,>=1.22.4 in /usr/local/lib/python3.10/dist-packages (from scipy->torch-sparse) (1.26.4)\n",
            "Installing collected packages: torch-sparse\n",
            "Successfully installed torch-sparse-0.6.18+pt25cu121\n",
            "Looking in links: https://data.pyg.org/whl/torch-2.5.1+cu121.html\n",
            "Collecting torch-cluster\n",
            "  Downloading https://data.pyg.org/whl/torch-2.5.0%2Bcu121/torch_cluster-1.6.3%2Bpt25cu121-cp310-cp310-linux_x86_64.whl (3.4 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.4/3.4 MB\u001b[0m \u001b[31m20.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: scipy in /usr/local/lib/python3.10/dist-packages (from torch-cluster) (1.13.1)\n",
            "Requirement already satisfied: numpy<2.3,>=1.22.4 in /usr/local/lib/python3.10/dist-packages (from scipy->torch-cluster) (1.26.4)\n",
            "Installing collected packages: torch-cluster\n",
            "Successfully installed torch-cluster-1.6.3+pt25cu121\n",
            "Looking in links: https://data.pyg.org/whl/torch-2.5.1+cu121.html\n",
            "Collecting torch-spline-conv\n",
            "  Downloading https://data.pyg.org/whl/torch-2.5.0%2Bcu121/torch_spline_conv-1.2.2%2Bpt25cu121-cp310-cp310-linux_x86_64.whl (991 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m991.6/991.6 kB\u001b[0m \u001b[31m18.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: torch-spline-conv\n",
            "Successfully installed torch-spline-conv-1.2.2+pt25cu121\n",
            "Looking in links: https://data.pyg.org/whl/torch-2.5.1+cu121.html\n",
            "Collecting torch-scatter\n",
            "  Downloading https://data.pyg.org/whl/torch-2.5.0%2Bcu121/torch_scatter-2.1.2%2Bpt25cu121-cp310-cp310-linux_x86_64.whl (10.9 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m10.9/10.9 MB\u001b[0m \u001b[31m49.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: torch-scatter\n",
            "Successfully installed torch-scatter-2.1.2+pt25cu121\n",
            "Collecting torch-geometric\n",
            "  Downloading torch_geometric-2.6.1-py3-none-any.whl.metadata (63 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m63.1/63.1 kB\u001b[0m \u001b[31m2.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: aiohttp in /usr/local/lib/python3.10/dist-packages (from torch-geometric) (3.11.10)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch-geometric) (2024.10.0)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch-geometric) (3.1.4)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from torch-geometric) (1.26.4)\n",
            "Requirement already satisfied: psutil>=5.8.0 in /usr/local/lib/python3.10/dist-packages (from torch-geometric) (5.9.5)\n",
            "Requirement already satisfied: pyparsing in /usr/local/lib/python3.10/dist-packages (from torch-geometric) (3.2.0)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from torch-geometric) (2.32.3)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from torch-geometric) (4.67.1)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->torch-geometric) (2.4.4)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp->torch-geometric) (1.3.2)\n",
            "Requirement already satisfied: async-timeout<6.0,>=4.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->torch-geometric) (4.0.3)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->torch-geometric) (24.3.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp->torch-geometric) (1.5.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp->torch-geometric) (6.1.0)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->torch-geometric) (0.2.1)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->torch-geometric) (1.18.3)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch-geometric) (3.0.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->torch-geometric) (3.4.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->torch-geometric) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->torch-geometric) (2.2.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->torch-geometric) (2024.12.14)\n",
            "Requirement already satisfied: typing-extensions>=4.1.0 in /usr/local/lib/python3.10/dist-packages (from multidict<7.0,>=4.5->aiohttp->torch-geometric) (4.12.2)\n",
            "Downloading torch_geometric-2.6.1-py3-none-any.whl (1.1 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.1/1.1 MB\u001b[0m \u001b[31m24.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: torch-geometric\n",
            "Successfully installed torch-geometric-2.6.1\n"
          ]
        }
      ],
      "source": [
        "!pip install torch-sparse -f https://data.pyg.org/whl/torch-$(python -c \"import torch; print(torch.__version__)\").html\n",
        "!pip install torch-cluster -f https://data.pyg.org/whl/torch-$(python -c \"import torch; print(torch.__version__)\").html\n",
        "!pip install torch-spline-conv -f https://data.pyg.org/whl/torch-$(python -c \"import torch; print(torch.__version__)\").html\n",
        "!pip install torch-scatter -f https://data.pyg.org/whl/torch-$(python -c \"import torch; print(torch.__version__)\").html\n",
        "!pip install torch-geometric"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "R_aIthCUzEpc",
        "outputId": "0c9bf997-dbd4-4209-b173-4be35d3a0520"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'mamba'...\n",
            "remote: Enumerating objects: 707, done.\u001b[K\n",
            "remote: Counting objects: 100% (13/13), done.\u001b[K\n",
            "remote: Compressing objects: 100% (6/6), done.\u001b[K\n",
            "remote: Total 707 (delta 10), reused 7 (delta 7), pack-reused 694 (from 2)\u001b[K\n",
            "Receiving objects: 100% (707/707), 1.55 MiB | 18.50 MiB/s, done.\n",
            "Resolving deltas: 100% (376/376), done.\n"
          ]
        }
      ],
      "source": [
        "! git clone https://github.com/state-spaces/mamba"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EdlvuG5_zGh8",
        "outputId": "007d6aa9-de96-40de-abed-58bb07bf10ac"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processing ./mamba\n",
            "  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.10/dist-packages (from mamba_ssm==2.2.4) (2.5.1+cu121)\n",
            "Collecting ninja (from mamba_ssm==2.2.4)\n",
            "  Using cached ninja-1.11.1.3-py3-none-manylinux_2_12_x86_64.manylinux2010_x86_64.whl.metadata (5.3 kB)\n",
            "Requirement already satisfied: einops in /usr/local/lib/python3.10/dist-packages (from mamba_ssm==2.2.4) (0.8.0)\n",
            "Requirement already satisfied: transformers in /usr/local/lib/python3.10/dist-packages (from mamba_ssm==2.2.4) (4.47.1)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from mamba_ssm==2.2.4) (24.2)\n",
            "Requirement already satisfied: setuptools>=61.0.0 in /usr/local/lib/python3.10/dist-packages (from mamba_ssm==2.2.4) (75.1.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch->mamba_ssm==2.2.4) (3.16.1)\n",
            "Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.10/dist-packages (from torch->mamba_ssm==2.2.4) (4.12.2)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch->mamba_ssm==2.2.4) (3.4.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch->mamba_ssm==2.2.4) (3.1.4)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch->mamba_ssm==2.2.4) (2024.10.0)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.10/dist-packages (from torch->mamba_ssm==2.2.4) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy==1.13.1->torch->mamba_ssm==2.2.4) (1.3.0)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.24.0 in /usr/local/lib/python3.10/dist-packages (from transformers->mamba_ssm==2.2.4) (0.27.0)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from transformers->mamba_ssm==2.2.4) (1.26.4)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers->mamba_ssm==2.2.4) (6.0.2)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers->mamba_ssm==2.2.4) (2024.11.6)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers->mamba_ssm==2.2.4) (2.32.3)\n",
            "Requirement already satisfied: tokenizers<0.22,>=0.21 in /usr/local/lib/python3.10/dist-packages (from transformers->mamba_ssm==2.2.4) (0.21.0)\n",
            "Requirement already satisfied: safetensors>=0.4.1 in /usr/local/lib/python3.10/dist-packages (from transformers->mamba_ssm==2.2.4) (0.4.5)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.10/dist-packages (from transformers->mamba_ssm==2.2.4) (4.67.1)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch->mamba_ssm==2.2.4) (3.0.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->transformers->mamba_ssm==2.2.4) (3.4.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers->mamba_ssm==2.2.4) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers->mamba_ssm==2.2.4) (2.2.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers->mamba_ssm==2.2.4) (2024.12.14)\n",
            "Using cached ninja-1.11.1.3-py3-none-manylinux_2_12_x86_64.manylinux2010_x86_64.whl (422 kB)\n",
            "Building wheels for collected packages: mamba_ssm\n",
            "  Building wheel for mamba_ssm (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for mamba_ssm: filename=mamba_ssm-2.2.4-cp310-cp310-linux_x86_64.whl size=323653202 sha256=c6edff068928b4ceacc6e820a163908c02294fe01dbcc05e95ed4530a5f81e77\n",
            "  Stored in directory: /tmp/pip-ephem-wheel-cache-g3wz8h00/wheels/fa/fd/f2/375cfbc979bce4a65dc40a325c0f7b1dc7064b0857ea7ae56b\n",
            "Successfully built mamba_ssm\n",
            "Installing collected packages: ninja, mamba_ssm\n",
            "Successfully installed mamba_ssm-2.2.4 ninja-1.11.1.3\n"
          ]
        }
      ],
      "source": [
        "!pip install ./mamba"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VopvhA-401hW",
        "outputId": "492283af-564f-49b2-d75e-0ce1232c1436"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting triton\n",
            "  Using cached triton-3.1.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (1.3 kB)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from triton) (3.16.1)\n",
            "Using cached triton-3.1.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (209.5 MB)\n",
            "Installing collected packages: triton\n",
            "Successfully installed triton-3.1.0\n"
          ]
        }
      ],
      "source": [
        "! pip install triton"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "9VDvU6KpuYOm"
      },
      "outputs": [],
      "source": [
        "import argparse\n",
        "import os.path as osp\n",
        "from typing import Any, Dict, Optional\n",
        "import math\n",
        "import torch as th\n",
        "\n",
        "import torch\n",
        "from torch.nn import (\n",
        "    BatchNorm1d,\n",
        "    Embedding,\n",
        "    Linear,\n",
        "    ModuleList,\n",
        "    ReLU,\n",
        "    Sequential,\n",
        ")\n",
        "from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
        "import gc\n",
        "import torch_geometric.transforms as T\n",
        "from torch_geometric.datasets import ZINC\n",
        "from torch_geometric.loader import DataLoader\n",
        "from torch_geometric.nn import GINEConv, global_add_pool\n",
        "import inspect\n",
        "from typing import Any, Dict, Optional\n",
        "\n",
        "import torch.nn.functional as F\n",
        "from torch import Tensor\n",
        "from torch.nn import Dropout, Linear, Sequential\n",
        "\n",
        "from torch_geometric.nn.conv import MessagePassing\n",
        "from torch_geometric.nn.inits import reset\n",
        "from torch_geometric.nn.resolver import (\n",
        "    activation_resolver,\n",
        "    normalization_resolver,\n",
        ")\n",
        "from torch_geometric.typing import Adj\n",
        "from torch_geometric.utils import to_dense_batch\n",
        "\n",
        "from mamba_ssm import Mamba\n",
        "from torch_geometric.utils import degree, sort_edge_index\n",
        "import torch_sparse"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "q5EyeFBD1pJg",
        "outputId": "abcaae88-99c1-4d23-cb26-01e57d70e720"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Original x: tensor([ 0, 10, 20, 30, 40, 50, 60])\n",
            "Permuted x: tensor([ 0, 20, 10, 30, 60, 50, 40])\n",
            "Permuted indices: tensor([0, 2, 1, 3, 6, 5, 4])\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "\n",
        "def permute_within_batch(x, batch):\n",
        "    # Enumerate over unique batch indices\n",
        "    unique_batches = torch.unique(batch)\n",
        "\n",
        "    # Initialize list to store permuted indices\n",
        "    permuted_indices = []\n",
        "\n",
        "    for batch_index in unique_batches:\n",
        "        # Extract indices for the current batch\n",
        "        indices_in_batch = (batch == batch_index).nonzero().squeeze()\n",
        "\n",
        "        # Permute indices within the current batch\n",
        "        permuted_indices_in_batch = indices_in_batch[torch.randperm(len(indices_in_batch))]\n",
        "\n",
        "        # Append permuted indices to the list\n",
        "        permuted_indices.append(permuted_indices_in_batch)\n",
        "\n",
        "    # Concatenate permuted indices into a single tensor\n",
        "    permuted_indices = torch.cat(permuted_indices)\n",
        "\n",
        "    return permuted_indices\n",
        "\n",
        "# Example usage\n",
        "batch = torch.tensor([0, 0, 0, 1, 1, 1, 1])\n",
        "x = torch.tensor([0, 10, 20, 30, 40, 50, 60])\n",
        "\n",
        "# Get permuted indices\n",
        "permuted_indices = permute_within_batch(x, batch)\n",
        "\n",
        "# Use permuted indices to get the permuted tensor\n",
        "permuted_x = x[permuted_indices]\n",
        "\n",
        "print(\"Original x:\", x)\n",
        "print(\"Permuted x:\", permuted_x)\n",
        "print(\"Permuted indices:\", permuted_indices)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "PtCJx6iE4ycm"
      },
      "outputs": [],
      "source": [
        "def timestep_embedding(timesteps, dim, max_period=10000):\n",
        "      \"\"\"\n",
        "    Create sinusoidal timestep embeddings.\n",
        "\n",
        "    :param timesteps: a 1-D Tensor of N indices, one per batch element.\n",
        "                      These may be fractional.\n",
        "    :param dim: the dimension of the output.\n",
        "    :param max_period: controls the minimum frequency of the embeddings.\n",
        "    :return: an [N x dim] Tensor of positional embeddings.\n",
        "    \"\"\"\n",
        "      half = dim // 2\n",
        "      freqs = th.exp(\n",
        "        -math.log(max_period) * th.arange(start=0, end=half, dtype=th.float32) / half\n",
        "      ).to(device=timesteps.device)\n",
        "      args = timesteps[:, None].float() * freqs[None]\n",
        "      embedding = th.cat([th.cos(args), th.sin(args)], dim=-1)\n",
        "      if dim % 2:\n",
        "        embedding = th.cat([embedding, th.zeros_like(embedding[:, :1])], dim=-1)\n",
        "      return embedding"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "EpyxpYsm1LhK"
      },
      "outputs": [],
      "source": [
        "class GPSConv(torch.nn.Module):\n",
        "\n",
        "    def __init__(\n",
        "        self,\n",
        "        channels: int,\n",
        "        conv: Optional[MessagePassing],\n",
        "        heads: int = 1,\n",
        "        dropout: float = 0.0,\n",
        "        attn_dropout: float = 0.0,\n",
        "        act: str = 'relu',\n",
        "        att_type: str = 'transformer',\n",
        "        order_by_degree: bool = False,\n",
        "        shuffle_ind: int = 0,\n",
        "        d_state: int = 16,\n",
        "        d_conv: int = 4,\n",
        "        act_kwargs: Optional[Dict[str, Any]] = None,\n",
        "        norm: Optional[str] = 'batch_norm',\n",
        "        norm_kwargs: Optional[Dict[str, Any]] = None,\n",
        "    ):\n",
        "        super().__init__()\n",
        "\n",
        "        self.channels = channels\n",
        "        self.conv = conv\n",
        "        self.heads = heads\n",
        "        self.dropout = dropout\n",
        "        self.att_type = att_type\n",
        "        self.shuffle_ind = shuffle_ind\n",
        "        self.order_by_degree = order_by_degree\n",
        "\n",
        "        assert (self.order_by_degree==True and self.shuffle_ind==0) or (self.order_by_degree==False), f'order_by_degree={self.order_by_degree} and shuffle_ind={self.shuffle_ind}'\n",
        "        if self.att_type == 'transformer':\n",
        "            self.attn = torch.nn.MultiheadAttention(\n",
        "                channels,\n",
        "                heads,\n",
        "                dropout=attn_dropout,\n",
        "                batch_first=True,\n",
        "            )\n",
        "        if self.att_type == 'mamba':\n",
        "            self.self_attn = Mamba(\n",
        "                d_model=channels,\n",
        "                d_state=d_state,\n",
        "                d_conv=d_conv,\n",
        "                expand=1\n",
        "            )\n",
        "\n",
        "        self.mlp = Sequential(\n",
        "            Linear(channels, channels * 2),\n",
        "            activation_resolver(act, **(act_kwargs or {})),\n",
        "            Dropout(dropout),\n",
        "            Linear(channels * 2, channels),\n",
        "            Dropout(dropout),\n",
        "        )\n",
        "\n",
        "        norm_kwargs = norm_kwargs or {}\n",
        "        self.norm1 = normalization_resolver(norm, channels, **norm_kwargs)\n",
        "        self.norm2 = normalization_resolver(norm, channels, **norm_kwargs)\n",
        "        self.norm3 = normalization_resolver(norm, channels, **norm_kwargs)\n",
        "\n",
        "        self.norm_with_batch = False\n",
        "        if self.norm1 is not None:\n",
        "            signature = inspect.signature(self.norm1.forward)\n",
        "            self.norm_with_batch = 'batch' in signature.parameters\n",
        "\n",
        "    def reset_parameters(self):\n",
        "        r\"\"\"Resets all learnable parameters of the module.\"\"\"\n",
        "        if self.conv is not None:\n",
        "            self.conv.reset_parameters()\n",
        "        self.attn._reset_parameters()\n",
        "        reset(self.mlp)\n",
        "        if self.norm1 is not None:\n",
        "            self.norm1.reset_parameters()\n",
        "        if self.norm2 is not None:\n",
        "            self.norm2.reset_parameters()\n",
        "        if self.norm3 is not None:\n",
        "            self.norm3.reset_parameters()\n",
        "\n",
        "    def forward(\n",
        "        self,\n",
        "        h: Tensor,\n",
        "        x: Tensor,\n",
        "        edge_index: Adj,\n",
        "        batch: Optional[torch.Tensor] = None,\n",
        "        **kwargs,\n",
        "    ) -> Tensor:\n",
        "        r\"\"\"Runs the forward pass of the module.\"\"\"\n",
        "        hs = []\n",
        "        if self.conv is not None:  # Local MPNN.\n",
        "            y,x = self.conv(h, x, **kwargs)\n",
        "            y = F.dropout(h, p=self.dropout, training=self.training)\n",
        "            y = y + h\n",
        "            if self.norm1 is not None:\n",
        "                if self.norm_with_batch:\n",
        "                    y = self.norm1(y, batch=batch)\n",
        "                else:\n",
        "                    y = self.norm1(y)\n",
        "            hs.append(y)\n",
        "            #print(\"shape of hs from MPNN:\", h.shape)\n",
        "\n",
        "        ### Global attention transformer-style model.\n",
        "        if self.att_type == 'transformer':\n",
        "            y, mask = to_dense_batch(h, batch)\n",
        "            y, _ = self.attn(y, y, y, key_padding_mask=~mask, need_weights=False)\n",
        "            h = h[mask]\n",
        "\n",
        "        if self.att_type == 'mamba':\n",
        "\n",
        "            if self.order_by_degree:\n",
        "              # degree() function: edge_index[0]= source nodes of the edges, x.shape[0]= number of nodes)\n",
        "                deg = degree(edge_index[0], h.shape[0]).to(torch.long)\n",
        "                order_tensor = torch.stack([batch, deg], 1).T\n",
        "                # creates a tensor by stacking the 'batch' tensor(batch_index of each node) and the degree of each node. making it easier to reorder nodes\n",
        "                _, h = sort_edge_index(order_tensor, edge_attr=h)\n",
        "                # reorders the feature tensor x based on the node degree and batch information, reordering x as well.\n",
        "\n",
        "            if self.shuffle_ind == 0:\n",
        "              # no shuffling occurs, and dense batch repr. is created from x\n",
        "                y, mask = to_dense_batch(h, batch)\n",
        "                y = self.self_attn(y)[mask]\n",
        "                # the result is masked to remove invalid entries (padded entries)\n",
        "                #\n",
        "            else:\n",
        "                mamba_arr = []\n",
        "                # list to store the results of multiple attention passes over shuffled node features\n",
        "                for _ in range(self.shuffle_ind):\n",
        "                  # loops over the number of shuffles(shuffle_ind)\n",
        "                    y_ind_perm = permute_within_batch(h, batch)\n",
        "                    # Similar to before, converts the shuffled x into a dense representation for each bat\n",
        "                    y_i, mask = to_dense_batch(h[y_ind_perm], batch)\n",
        "                    # Similar to before, converts the shuffled x into a dense representation for each batch.\n",
        "                    y_i = self.self_attn(y_i)[mask][y_ind_perm]\n",
        "                    # The shuffled dense batch is passed through the self-attention mechanism, and the result is reordered back to the original node order (h_ind_perm).\n",
        "                    mamba_arr.append(y_i)\n",
        "                y = sum(mamba_arr) / self.shuffle_ind\n",
        "                #print(\"Shape of h from mamba:\", y.shape)\n",
        "\n",
        "                # Averages the results from all shuffled versions to produce the final node representations.\n",
        "        ###\n",
        "\n",
        "        y = F.dropout(y, p=self.dropout, training=self.training)\n",
        "        y = y + h  # Residual connection.\n",
        "        if self.norm2 is not None:\n",
        "            if self.norm_with_batch:\n",
        "                y = self.norm2(y, batch=batch)\n",
        "            else:\n",
        "                y = self.norm2(y)\n",
        "        hs.append(y)\n",
        "        #print(\"shape of hs from attention:\", h.shape)\n",
        "\n",
        "        out = sum(hs)  # Combine local and global outputs.\n",
        "        #print(\"Shape of out:\", out.shape)\n",
        "\n",
        "        out = out + self.mlp(out)\n",
        "        if self.norm3 is not None:\n",
        "            if self.norm_with_batch:\n",
        "                out = self.norm3(out, batch=batch)\n",
        "            else:\n",
        "                out = self.norm3(out)\n",
        "\n",
        "        return out\n",
        "\n",
        "    def __repr__(self) -> str:\n",
        "        return (f'{self.__class__.__name__}({self.channels}, '\n",
        "                f'conv={self.conv}, heads={self.heads})')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "81fuS3-R2UEy"
      },
      "outputs": [],
      "source": [
        "class GraphModel(torch.nn.Module):\n",
        "    def __init__(self, channels: int, in_node_nf: int, out_node_nf: int, in_edge_nf: int, hidden_nf: int, pe_dim: int, context_dim: int, time_embed_dim: int, num_layers: int, model_type: str, shuffle_ind: int, d_state: int, d_conv: int, order_by_degree: False):\n",
        "        super().__init__()\n",
        "\n",
        "        self.node_emb = Linear(64, channels - pe_dim - time_embed_dim - context_dim)\n",
        "        self.context_emb = Linear(64, context_dim)\n",
        "        self.context_dim = context_dim\n",
        "        self.time_embed_dim = time_embed_dim\n",
        "        self.pe_lin = Linear(20, pe_dim)    # PE_encoder = linear, DeepSet\n",
        "        self.pe_norm = BatchNorm1d(20)\n",
        "        self.edge_emb = Linear(4, channels-time_embed_dim)\n",
        "        self.model_type = model_type\n",
        "        self.shuffle_ind = shuffle_ind\n",
        "        self.order_by_degree = order_by_degree\n",
        "        self.convs = ModuleList()\n",
        "\n",
        "        for _ in range(num_layers):\n",
        "            nn = Sequential(\n",
        "                Linear(channels, channels),\n",
        "                ReLU(),\n",
        "                Linear(channels, channels),\n",
        "            )\n",
        "            if self.model_type == 'egnn':\n",
        "                conv = eg.EGNN(in_node_nf, hidden_nf, out_node_nf, in_edge_nf)\n",
        "\n",
        "            if self.model_type == 'mamba':\n",
        "                conv = GPSConv(channels, eg.EGNN(in_node_nf, hidden_nf, out_node_nf, in_edge_nf), heads=4, attn_dropout=0.5,\n",
        "                               att_type='mamba',\n",
        "                               shuffle_ind=self.shuffle_ind,\n",
        "                               order_by_degree=self.order_by_degree,\n",
        "                               d_state=d_state, d_conv=d_conv)\n",
        "\n",
        "            if self.model_type == 'transformer':\n",
        "                conv = GPSConv(channels, eg.EGNN(in_node_nf, hidden_nf, out_node_nf, in_edge_nf), heads=4, attn_dropout=0.5, att_type='transformer')\n",
        "            self.convs.append(conv)\n",
        "\n",
        "        self.mlp = Sequential(\n",
        "            Linear(channels, channels // 2),\n",
        "            ReLU(),\n",
        "            Linear(channels // 2, channels // 4),\n",
        "            ReLU(),\n",
        "            Linear(channels // 4, 1),\n",
        "        )\n",
        "\n",
        "    def forward(self, h, pe, x, t, context, edges, edge_index, edge_attr, batch):\n",
        "\n",
        "        h_pe = self.pe_norm(pe)        # positional_encoding = RWSE-20, LapPE-8, LapPE-16\n",
        "        time_emb = timestep_embedding(t, self.time_embed_dim)\n",
        "        context = self.context_emb(context)\n",
        "        h = torch.cat((self.node_emb(h.squeeze(-1)), self.pe_lin(h_pe), time_emb.squeeze(1), context), dim=1)\n",
        "\n",
        "        edge_attr = self.edge_emb(edge_attr.squeeze(-1))\n",
        "        required_repeats = edge_attr.size(0) // time_emb.size(0)\n",
        "        remainder = edge_attr.size(0) % time_emb.size(0)\n",
        "        time_emb_edge = time_emb.repeat_interleave(required_repeats, dim=0)\n",
        "\n",
        "        if remainder > 0:\n",
        "          extra_time_emb = time_emb[:remainder]\n",
        "        time_emb_edge = torch.cat((time_emb_edge, extra_time_emb), dim=0)\n",
        "\n",
        "        assert time_emb_edge.size(0) == edge_attr.size(0), \"Final sizes still do not match!\"\n",
        "        edge_attr = torch.cat((edge_attr, time_emb_edge.squeeze(1)), dim=-1)\n",
        "\n",
        "        for conv in self.convs:\n",
        "            if self.model_type == 'egnn':\n",
        "                h = conv(h, x, edges=edges, edge_attr=edge_attr)\n",
        "            else:\n",
        "                h = conv(h,x, edge_index, batch, edges=edges, edge_attr=edge_attr)\n",
        "        h = global_add_pool(h, batch)    # graph_pool = sum, mean\n",
        "        return self.mlp(h)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fVWhktPRlCIG",
        "outputId": "4ed68b42-8642-42ca-c6a6-b0bba55a8e55"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "GraphModel(\n",
            "  (node_emb): Linear(in_features=64, out_features=16, bias=True)\n",
            "  (context_emb): Linear(in_features=64, out_features=16, bias=True)\n",
            "  (pe_lin): Linear(in_features=20, out_features=16, bias=True)\n",
            "  (pe_norm): BatchNorm1d(20, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "  (edge_emb): Linear(in_features=4, out_features=48, bias=True)\n",
            "  (convs): ModuleList(\n",
            "    (0-2): 3 x GPSConv(64, conv=EGNN(\n",
            "      (embedding_in): Linear(in_features=64, out_features=32, bias=True)\n",
            "      (embedding_out): Linear(in_features=32, out_features=64, bias=True)\n",
            "      (gcl_0): E_GCL(\n",
            "        (edge_mlp): Sequential(\n",
            "          (0): Linear(in_features=129, out_features=32, bias=True)\n",
            "          (1): SiLU()\n",
            "          (2): Linear(in_features=32, out_features=32, bias=True)\n",
            "          (3): SiLU()\n",
            "        )\n",
            "        (node_mlp): Sequential(\n",
            "          (0): Linear(in_features=64, out_features=32, bias=True)\n",
            "          (1): SiLU()\n",
            "          (2): Linear(in_features=32, out_features=32, bias=True)\n",
            "        )\n",
            "        (coord_mlp): Sequential(\n",
            "          (0): Linear(in_features=32, out_features=32, bias=True)\n",
            "          (1): SiLU()\n",
            "          (2): Linear(in_features=32, out_features=1, bias=False)\n",
            "        )\n",
            "      )\n",
            "      (gcl_1): E_GCL(\n",
            "        (edge_mlp): Sequential(\n",
            "          (0): Linear(in_features=129, out_features=32, bias=True)\n",
            "          (1): SiLU()\n",
            "          (2): Linear(in_features=32, out_features=32, bias=True)\n",
            "          (3): SiLU()\n",
            "        )\n",
            "        (node_mlp): Sequential(\n",
            "          (0): Linear(in_features=64, out_features=32, bias=True)\n",
            "          (1): SiLU()\n",
            "          (2): Linear(in_features=32, out_features=32, bias=True)\n",
            "        )\n",
            "        (coord_mlp): Sequential(\n",
            "          (0): Linear(in_features=32, out_features=32, bias=True)\n",
            "          (1): SiLU()\n",
            "          (2): Linear(in_features=32, out_features=1, bias=False)\n",
            "        )\n",
            "      )\n",
            "      (gcl_2): E_GCL(\n",
            "        (edge_mlp): Sequential(\n",
            "          (0): Linear(in_features=129, out_features=32, bias=True)\n",
            "          (1): SiLU()\n",
            "          (2): Linear(in_features=32, out_features=32, bias=True)\n",
            "          (3): SiLU()\n",
            "        )\n",
            "        (node_mlp): Sequential(\n",
            "          (0): Linear(in_features=64, out_features=32, bias=True)\n",
            "          (1): SiLU()\n",
            "          (2): Linear(in_features=32, out_features=32, bias=True)\n",
            "        )\n",
            "        (coord_mlp): Sequential(\n",
            "          (0): Linear(in_features=32, out_features=32, bias=True)\n",
            "          (1): SiLU()\n",
            "          (2): Linear(in_features=32, out_features=1, bias=False)\n",
            "        )\n",
            "      )\n",
            "      (gcl_3): E_GCL(\n",
            "        (edge_mlp): Sequential(\n",
            "          (0): Linear(in_features=129, out_features=32, bias=True)\n",
            "          (1): SiLU()\n",
            "          (2): Linear(in_features=32, out_features=32, bias=True)\n",
            "          (3): SiLU()\n",
            "        )\n",
            "        (node_mlp): Sequential(\n",
            "          (0): Linear(in_features=64, out_features=32, bias=True)\n",
            "          (1): SiLU()\n",
            "          (2): Linear(in_features=32, out_features=32, bias=True)\n",
            "        )\n",
            "        (coord_mlp): Sequential(\n",
            "          (0): Linear(in_features=32, out_features=32, bias=True)\n",
            "          (1): SiLU()\n",
            "          (2): Linear(in_features=32, out_features=1, bias=False)\n",
            "        )\n",
            "      )\n",
            "    ), heads=4)\n",
            "  )\n",
            "  (mlp): Sequential(\n",
            "    (0): Linear(in_features=64, out_features=32, bias=True)\n",
            "    (1): ReLU()\n",
            "    (2): Linear(in_features=32, out_features=16, bias=True)\n",
            "    (3): ReLU()\n",
            "    (4): Linear(in_features=16, out_features=1, bias=True)\n",
            "  )\n",
            ")\n"
          ]
        }
      ],
      "source": [
        "# Define hyperparameters\n",
        "channels = 64\n",
        "in_node_nf = channels   # Example input node feature size\n",
        "out_node_nf = channels # Example output node feature size\n",
        "in_edge_nf = 64    # Example input edge feature size\n",
        "hidden_nf = 32   # Number of hidden units in layers\n",
        "pe_dim = 16\n",
        "time_embed_dim = 16\n",
        "context_dim = 16      # Positional encoding dimension\n",
        "num_layers = 3    # Number of layers in the model\n",
        "model_type = \"mamba\"  # Graph Neural Network as the type\n",
        "shuffle_ind = 1    # Enable index shuffling\n",
        "d_state = 64       # State dimensionality\n",
        "d_conv = 64        # Convolutional dimensionality\n",
        "order_by_degree = False  # Do not order nodes by degree\n",
        "\n",
        "# Initialize the model\n",
        "model = GraphModel(\n",
        "    channels=channels,\n",
        "    in_node_nf=in_node_nf,\n",
        "    out_node_nf=out_node_nf,\n",
        "    in_edge_nf=in_edge_nf,\n",
        "    hidden_nf=hidden_nf,\n",
        "    pe_dim=pe_dim,\n",
        "    context_dim=context_dim,\n",
        "    time_embed_dim=time_embed_dim,\n",
        "    num_layers=num_layers,\n",
        "    model_type=model_type,\n",
        "    shuffle_ind=shuffle_ind,\n",
        "    d_state=d_state,\n",
        "    d_conv=d_conv,\n",
        "    order_by_degree=order_by_degree,\n",
        ").to(device)\n",
        "\n",
        "# Print model summary (if `__repr__` or similar is defined)\n",
        "print(model)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sQcsGWX7mLru"
      },
      "outputs": [],
      "source": [
        "#batch_size = 8\n",
        "#n_nodes = 4\n",
        "#n_feat = 64\n",
        "#x_dim = 3\n",
        "\n",
        "# Dummy variables h, x and fully connected edges\n",
        "#h = torch.ones(batch_size *  n_nodes, n_feat)\n",
        "#x = torch.ones(batch_size * n_nodes, x_dim)\n",
        "#pe = torch.rand((batch_size* n_nodes, 20))\n",
        "#batch = torch.zeros(batch_size * n_nodes, dtype=torch.long)\n",
        "#print(batch.shape)\n",
        "#edges, edge_attr = eg.get_edges_batch(n_nodes, batch_size)\n",
        "#print(h.shape)\n",
        "#print(x.shape)\n",
        "#print(pe.shape)\n",
        "#print(batch.shape)\n",
        "#print(edge_attr.shape)\n",
        "#edge_index = get_edge_index_batch(n_nodes, batch_size)\n",
        "# Initialize EGNN\n",
        "#egnn = eg.EGNN(in_node_nf=n_feat, hidden_nf=32, out_node_nf=64, in_edge_nf=1)\n",
        "\n",
        "# Run EGNN\n",
        "#h, x = egnn(h, x, edges, edge_attr)\n",
        "#print(h.shape, x.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "71uJryUN5xh_",
        "outputId": "4f165fac-ea40-4656-df45-6d8239833ff1"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[0.3427, 0.2709, 0.5347,  ..., 0.4272, 0.9020, 0.8976],\n",
            "        [0.2671, 0.5069, 0.6776,  ..., 0.9929, 0.8624, 0.4720],\n",
            "        [0.1337, 0.4303, 0.8587,  ..., 0.4891, 0.7263, 0.0137],\n",
            "        ...,\n",
            "        [0.2664, 0.5034, 0.8762,  ..., 0.0420, 0.6542, 0.4533],\n",
            "        [0.7293, 0.7885, 0.7095,  ..., 0.7060, 0.1579, 0.3635],\n",
            "        [0.2505, 0.8198, 0.4655,  ..., 0.1269, 0.4545, 0.5341]],\n",
            "       device='cuda:0')\n",
            "1\n",
            "torch.Size([31, 64])\n",
            "torch.Size([2, 66])\n",
            "torch.Size([66, 4])\n",
            "torch.Size([31, 3])\n",
            "torch.Size([31, 20])\n",
            "torch.Size([31])\n",
            "torch.Size([66])\n",
            "torch.Size([66])\n"
          ]
        }
      ],
      "source": [
        "n_nodes = 31\n",
        "n_edges = 66\n",
        "\n",
        "h = torch.rand((n_nodes, 64)).to(device)\n",
        "print(h)\n",
        "edge_index = torch.randint(0, n_nodes, (2, n_edges)).to(device)\n",
        "edge_attr = torch.rand((n_edges, 4)).to(device)\n",
        "edges = [edge_index[0].to(device), edge_index[1].to(device)]\n",
        "x = torch.rand((n_nodes, 3)).to(device)\n",
        "pe = torch.rand((n_nodes, 20)).to(device)\n",
        "batch = torch.zeros(n_nodes, dtype=torch.long).to(device)\n",
        "ptr = torch.tensor([0, n_nodes]).to(device)\n",
        "num_graphs = (ptr.shape[0]-1)\n",
        "print(num_graphs)\n",
        "print(h.shape)\n",
        "print(edge_index.shape)\n",
        "print(edge_attr.shape)\n",
        "print(x.shape)\n",
        "print(pe.shape)\n",
        "print(batch.shape)\n",
        "print(edges[0].shape)\n",
        "print(edges[1].shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "n5LvFd7w9Unl",
        "outputId": "ec785125-b720-401c-c13f-2ed7f3d7d58a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Shape of context before: torch.Size([31, 64])\n",
            "Context shape after repeating: torch.Size([31, 64])\n",
            "tensor([385.], device='cuda:0')\n",
            "torch.Size([31, 1])\n"
          ]
        }
      ],
      "source": [
        "context = torch.randn_like(h, dtype=torch.float).to(device)\n",
        "print(\"Shape of context before:\", context.shape)\n",
        "context = context.view(-1, context.size(-1))\n",
        "print(f\"Context shape after repeating: {context.shape}\")\n",
        "\n",
        "t = torch.randint(0, 1000, (num_graphs,)).float().to(device)\n",
        "print(t)\n",
        "t = t.view(-1, 1)\n",
        "t = t[batch]\n",
        "print(t.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KV2YM_9gmTQz",
        "outputId": "9b3d2d70-4bda-4728-d3a0-89843720c743"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([1, 1])\n"
          ]
        }
      ],
      "source": [
        "output = model( h, pe, x, t, context, edges, edge_index, edge_attr, batch)\n",
        "print(output.shape)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "deg = degree(edge_index[0], h.shape[0]).to(torch.long)\n",
        "print(deg)\n",
        "order_tensor = torch.stack([batch, deg], 1).T\n",
        "print(order_tensor)\n",
        "_, h = sort_edge_index(order_tensor, edge_attr=h)\n",
        "print(\"h after sorting by degree:\", h)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8RGgXACuj_s8",
        "outputId": "714181cb-7e7e-41ea-a1e2-ad51fd66eda7"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([1, 3, 2, 4, 1, 1, 1, 5, 3, 2, 2, 2, 1, 5, 2, 2, 3, 1, 2, 0, 2, 2, 2, 2,\n",
            "        3, 2, 2, 1, 5, 2, 0], device='cuda:0')\n",
            "tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "         0, 0, 0, 0, 0, 0, 0],\n",
            "        [1, 3, 2, 4, 1, 1, 1, 5, 3, 2, 2, 2, 1, 5, 2, 2, 3, 1, 2, 0, 2, 2, 2, 2,\n",
            "         3, 2, 2, 1, 5, 2, 0]], device='cuda:0')\n",
            "h after sorting by degree: tensor([[0.2505, 0.8198, 0.4655,  ..., 0.1269, 0.4545, 0.5341],\n",
            "        [0.1476, 0.3222, 0.3832,  ..., 0.7252, 0.4107, 0.7518],\n",
            "        [0.3841, 0.3592, 0.4825,  ..., 0.7634, 0.2679, 0.7684],\n",
            "        ...,\n",
            "        [0.5645, 0.4209, 0.2905,  ..., 0.6222, 0.7633, 0.0537],\n",
            "        [0.7457, 0.7225, 0.9545,  ..., 0.0978, 0.1752, 0.5202],\n",
            "        [0.2664, 0.5034, 0.8762,  ..., 0.0420, 0.6542, 0.4533]],\n",
            "       device='cuda:0')\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import networkx as nx\n",
        "import torch\n",
        "import random\n",
        "import numpy as np\n",
        "\n",
        "def prioritize_by_eigenvector_centrality(edge_index, num_nodes):\n",
        "    \"\"\"\n",
        "    Prioritize nodes based on eigenvector centrality.\n",
        "\n",
        "    Args:\n",
        "        edge_index (torch.Tensor): Edge index of the graph (shape [2, num_edges]).\n",
        "        num_nodes (int): Number of nodes in the graph.\n",
        "\n",
        "    Returns:\n",
        "        torch.Tensor: Tensor of node indices sorted by eigenvector centrality.\n",
        "    \"\"\"\n",
        "    # Create a NetworkX graph from edge_index\n",
        "    G = nx.Graph()\n",
        "    edges = edge_index.T.cpu().numpy()\n",
        "    G.add_edges_from(edges)\n",
        "\n",
        "    # Compute eigenvector centrality\n",
        "    centrality = nx.eigenvector_centrality_numpy(G)\n",
        "\n",
        "    # Get centrality scores and sort nodes\n",
        "    centrality_scores = torch.tensor(\n",
        "        [centrality[i] for i in range(num_nodes)], dtype=torch.float\n",
        "    )\n",
        "    #prioritized_nodes = torch.argsort(centrality_scores, descending=True)\n",
        "\n",
        "    return centrality_scores"
      ],
      "metadata": {
        "id": "nkRhXAxzkj-d"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import networkx as nx\n",
        "import torch\n",
        "import community.community_louvain as community_louvain\n",
        "\n",
        "def permute_by_clusters(edge_index, num_nodes):\n",
        "    \"\"\"\n",
        "    Permute nodes based on clusters defined by the Louvain algorithm.\n",
        "\n",
        "    Args:\n",
        "        edge_index (torch.Tensor): Edge index of the graph (shape [2, num_edges]).\n",
        "        num_nodes (int): Number of nodes in the graph.\n",
        "\n",
        "    Returns:\n",
        "        torch.Tensor: Permuted indices of nodes.\n",
        "    \"\"\"\n",
        "    # Convert to NetworkX graph\n",
        "    G = nx.Graph()\n",
        "    edges = edge_index.T.cpu().numpy()\n",
        "    G.add_edges_from(edges)\n",
        "\n",
        "    # Perform clustering using the Louvain algorithm\n",
        "    partition = community_louvain.best_partition(G)\n",
        "    print(\"Partition:\",partition)\n",
        "\n",
        "    # Group nodes by clusters\n",
        "    clusters = {}\n",
        "    for node, cluster_id in partition.items():\n",
        "        if cluster_id not in clusters:\n",
        "            clusters[cluster_id] = []\n",
        "        clusters[cluster_id].append(node)\n",
        "\n",
        "    # Shuffle nodes within each cluster and concatenate\n",
        "    permuted_indices = []\n",
        "    for cluster_nodes in clusters.values():\n",
        "        permuted_indices += torch.tensor(cluster_nodes)[torch.randperm(len(cluster_nodes))].tolist()\n",
        "\n",
        "    return torch.tensor(permuted_indices)"
      ],
      "metadata": {
        "id": "HE4WHqrhJz0k"
      },
      "execution_count": 36,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "centrality_scores = prioritize_by_eigenvector_centrality(edge_index, h.shape[0]).to(device)\n",
        "print(centrality_scores)\n",
        "order_tensor = torch.stack([batch, centrality_scores], 1).T\n",
        "print(order_tensor)\n",
        "_, h = sort_edge_index(order_tensor, edge_attr=h)\n",
        "print(\"h after sorting by degree:\", h)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pbdkYvvLlB_r",
        "outputId": "d2a878dc-2d9a-4876-e7cb-8451dba14ceb"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([0.0125, 0.1368, 0.1180, 0.3434, 0.1748, 0.0848, 0.2841, 0.3206, 0.1192,\n",
            "        0.2601, 0.1599, 0.1411, 0.0520, 0.2906, 0.0825, 0.1570, 0.1278, 0.0025,\n",
            "        0.1427, 0.0582, 0.0598, 0.0444, 0.1873, 0.1621, 0.0271, 0.2096, 0.2428,\n",
            "        0.0258, 0.3830, 0.1017, 0.1171], device='cuda:0')\n",
            "tensor([[0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
            "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
            "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
            "         0.0000, 0.0000, 0.0000, 0.0000],\n",
            "        [0.0125, 0.1368, 0.1180, 0.3434, 0.1748, 0.0848, 0.2841, 0.3206, 0.1192,\n",
            "         0.2601, 0.1599, 0.1411, 0.0520, 0.2906, 0.0825, 0.1570, 0.1278, 0.0025,\n",
            "         0.1427, 0.0582, 0.0598, 0.0444, 0.1873, 0.1621, 0.0271, 0.2096, 0.2428,\n",
            "         0.0258, 0.3830, 0.1017, 0.1171]], device='cuda:0')\n",
            "h after sorting by degree: tensor([[0.9710, 0.7370, 0.7639,  ..., 0.8878, 0.2152, 0.2150],\n",
            "        [0.2505, 0.8198, 0.4655,  ..., 0.1269, 0.4545, 0.5341],\n",
            "        [0.9901, 0.7185, 0.0095,  ..., 0.8437, 0.2239, 0.9538],\n",
            "        ...,\n",
            "        [0.2340, 0.4325, 0.6474,  ..., 0.4722, 0.3435, 0.9242],\n",
            "        [0.9934, 0.1722, 0.3096,  ..., 0.7963, 0.6018, 0.2183],\n",
            "        [0.5645, 0.4209, 0.2905,  ..., 0.6222, 0.7633, 0.0537]],\n",
            "       device='cuda:0')\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "mamba_arr = []\n",
        "shuffle_ind = 1\n",
        "for _ in range(shuffle_ind):\n",
        "  y_ind_perm = permute_within_batch(h, batch)\n",
        "  y_i, mask = to_dense_batch(h[y_ind_perm], batch)\n",
        "\n",
        "print(\"y_ind_perm:\", y_ind_perm)\n",
        "print(h[y_ind_perm])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cTVcrWIyIqUx",
        "outputId": "3ad1618b-efb8-4d5f-ccc9-d6b8e6d61dd2"
      },
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "y_ind_perm: tensor([ 0, 19, 21, 30,  8, 18,  5,  7, 29, 13,  1,  3,  2, 11, 20, 10,  9, 17,\n",
            "        14, 27, 26, 25, 23, 16, 28,  6, 24, 12, 15, 22,  4], device='cuda:0')\n",
            "tensor([[0.9710, 0.7370, 0.7639,  ..., 0.8878, 0.2152, 0.2150],\n",
            "        [0.5406, 0.7842, 0.9144,  ..., 0.6539, 0.4140, 0.1852],\n",
            "        [0.9816, 0.0697, 0.2098,  ..., 0.3765, 0.9406, 0.0673],\n",
            "        ...,\n",
            "        [0.1476, 0.3222, 0.3832,  ..., 0.7252, 0.4107, 0.7518],\n",
            "        [0.9161, 0.1328, 0.4911,  ..., 0.2706, 0.3738, 0.7482],\n",
            "        [0.1337, 0.4303, 0.8587,  ..., 0.4891, 0.7263, 0.0137]],\n",
            "       device='cuda:0')\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(h)\n",
        "permuted_indices = permute_by_clusters(edge_index, h.shape[0])\n",
        "assert permuted_indices.ndim == 1, \"Permuted indices should be a 1D tensor.\"\n",
        "# Use permuted indices to get the permuted tensor\n",
        "permuted_h = h[permuted_indices]\n",
        "print(\"Permuted h:\", permuted_h)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sfNesetSKoxD",
        "outputId": "37f57322-ba62-473d-c800-bcba030ec48d"
      },
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[ 0.8847,  0.9463, -1.1158,  ..., -0.5613, -1.1551, -0.1512],\n",
            "        [ 0.6538,  0.1757,  1.1467,  ..., -0.7878, -2.2434, -0.2146],\n",
            "        [ 0.5866,  1.2984,  0.3276,  ..., -0.9381, -0.2760, -0.2766],\n",
            "        ...,\n",
            "        [-1.3188, -0.5298,  0.0390,  ...,  1.3245, -0.5754,  0.1074],\n",
            "        [-1.4053,  0.9556,  0.5469,  ..., -1.7763,  0.3285,  0.9740],\n",
            "        [ 1.0622, -1.1467, -0.3092,  ...,  0.7172,  0.1658,  0.6781]])\n",
            "Partition: {0: 0, 1: 0, 2: 3, 3: 3, 4: 1, 5: 1, 6: 2, 7: 2}\n",
            "Permuted h: tensor([[ 0.8847,  0.9463, -1.1158,  ..., -0.5613, -1.1551, -0.1512],\n",
            "        [ 0.6538,  0.1757,  1.1467,  ..., -0.7878, -2.2434, -0.2146],\n",
            "        [ 0.5866,  1.2984,  0.3276,  ..., -0.9381, -0.2760, -0.2766],\n",
            "        ...,\n",
            "        [-0.0818, -0.7216, -0.7346,  ..., -0.4065, -1.7560, -0.2450],\n",
            "        [ 1.0622, -1.1467, -0.3092,  ...,  0.7172,  0.1658,  0.6781],\n",
            "        [-1.4053,  0.9556,  0.5469,  ..., -1.7763,  0.3285,  0.9740]])\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}