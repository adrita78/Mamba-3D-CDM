{
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "# device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(\"Device:\", device)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8HrNNQDaW1RU",
        "outputId": "82ba5c70-a98d-4b78-d15d-32b5a991119c"
      },
      "id": "8HrNNQDaW1RU",
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Device: cpu\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "! pip install mamba-ssm"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "p42rIz8OzzaV",
        "outputId": "397054b3-81b9-448a-c816-ea8589e90de0"
      },
      "id": "p42rIz8OzzaV",
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: mamba-ssm in /usr/local/lib/python3.10/dist-packages (2.2.2)\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.10/dist-packages (from mamba-ssm) (2.4.1+cu121)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from mamba-ssm) (24.1)\n",
            "Requirement already satisfied: ninja in /usr/local/lib/python3.10/dist-packages (from mamba-ssm) (1.11.1.1)\n",
            "Requirement already satisfied: einops in /usr/local/lib/python3.10/dist-packages (from mamba-ssm) (0.8.0)\n",
            "Requirement already satisfied: triton in /usr/local/lib/python3.10/dist-packages (from mamba-ssm) (3.1.0)\n",
            "Requirement already satisfied: transformers in /usr/local/lib/python3.10/dist-packages (from mamba-ssm) (4.44.2)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch->mamba-ssm) (3.16.1)\n",
            "Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.10/dist-packages (from torch->mamba-ssm) (4.12.2)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch->mamba-ssm) (1.13.3)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch->mamba-ssm) (3.4)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch->mamba-ssm) (3.1.4)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch->mamba-ssm) (2024.6.1)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.23.2 in /usr/local/lib/python3.10/dist-packages (from transformers->mamba-ssm) (0.24.7)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from transformers->mamba-ssm) (1.26.4)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers->mamba-ssm) (6.0.2)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers->mamba-ssm) (2024.9.11)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers->mamba-ssm) (2.32.3)\n",
            "Requirement already satisfied: safetensors>=0.4.1 in /usr/local/lib/python3.10/dist-packages (from transformers->mamba-ssm) (0.4.5)\n",
            "Requirement already satisfied: tokenizers<0.20,>=0.19 in /usr/local/lib/python3.10/dist-packages (from transformers->mamba-ssm) (0.19.1)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.10/dist-packages (from transformers->mamba-ssm) (4.66.5)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch->mamba-ssm) (3.0.1)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->transformers->mamba-ssm) (3.4.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers->mamba-ssm) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers->mamba-ssm) (2.2.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers->mamba-ssm) (2024.8.30)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy->torch->mamba-ssm) (1.3.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "print(torch.__version__)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1wNOJMBK4CX1",
        "outputId": "c2d91d85-742c-4d15-ac5f-8f6be2b6daad"
      },
      "id": "1wNOJMBK4CX1",
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2.4.1+cu121\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "id": "4cfb810b-2979-432a-9186-3a03f6ae086c",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4cfb810b-2979-432a-9186-3a03f6ae086c",
        "outputId": "9f10be72-64ba-476c-a286-d03e27f32cd8"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/mamba_ssm/ops/selective_scan_interface.py:164: FutureWarning: `torch.cuda.amp.custom_fwd(args...)` is deprecated. Please use `torch.amp.custom_fwd(args..., device_type='cuda')` instead.\n",
            "  def forward(ctx, xz, conv1d_weight, conv1d_bias, x_proj_weight, delta_proj_weight,\n",
            "/usr/local/lib/python3.10/dist-packages/mamba_ssm/ops/selective_scan_interface.py:240: FutureWarning: `torch.cuda.amp.custom_bwd(args...)` is deprecated. Please use `torch.amp.custom_bwd(args..., device_type='cuda')` instead.\n",
            "  def backward(ctx, dout):\n",
            "/usr/local/lib/python3.10/dist-packages/mamba_ssm/ops/triton/layer_norm.py:986: FutureWarning: `torch.cuda.amp.custom_fwd(args...)` is deprecated. Please use `torch.amp.custom_fwd(args..., device_type='cuda')` instead.\n",
            "  def forward(\n",
            "/usr/local/lib/python3.10/dist-packages/mamba_ssm/ops/triton/layer_norm.py:1045: FutureWarning: `torch.cuda.amp.custom_bwd(args...)` is deprecated. Please use `torch.amp.custom_bwd(args..., device_type='cuda')` instead.\n",
            "  def backward(ctx, dout, *args):\n",
            "/usr/local/lib/python3.10/dist-packages/mamba_ssm/distributed/tensor_parallel.py:26: FutureWarning: `torch.cuda.amp.custom_fwd(args...)` is deprecated. Please use `torch.amp.custom_fwd(args..., device_type='cuda')` instead.\n",
            "  def forward(ctx, x, weight, bias, process_group=None, sequence_parallel=True):\n",
            "/usr/local/lib/python3.10/dist-packages/mamba_ssm/distributed/tensor_parallel.py:62: FutureWarning: `torch.cuda.amp.custom_bwd(args...)` is deprecated. Please use `torch.amp.custom_bwd(args..., device_type='cuda')` instead.\n",
            "  def backward(ctx, grad_output):\n",
            "/usr/local/lib/python3.10/dist-packages/mamba_ssm/ops/triton/ssd_combined.py:758: FutureWarning: `torch.cuda.amp.custom_fwd(args...)` is deprecated. Please use `torch.amp.custom_fwd(args..., device_type='cuda')` instead.\n",
            "  def forward(ctx, zxbcdt, conv1d_weight, conv1d_bias, dt_bias, A, D, chunk_size, initial_states=None, seq_idx=None, dt_limit=(0.0, float(\"inf\")), return_final_states=False, activation=\"silu\",\n",
            "/usr/local/lib/python3.10/dist-packages/mamba_ssm/ops/triton/ssd_combined.py:836: FutureWarning: `torch.cuda.amp.custom_bwd(args...)` is deprecated. Please use `torch.amp.custom_bwd(args..., device_type='cuda')` instead.\n",
            "  def backward(ctx, dout, *args):\n"
          ]
        }
      ],
      "source": [
        "import argparse\n",
        "import os.path as osp\n",
        "from typing import Any, Dict, Optional\n",
        "\n",
        "import torch\n",
        "from torch.nn import (\n",
        "    BatchNorm1d,\n",
        "    Embedding,\n",
        "    Linear,\n",
        "    ModuleList,\n",
        "    ReLU,\n",
        "    Sequential,\n",
        ")\n",
        "from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
        "\n",
        "import torch_geometric.transforms as T\n",
        "from torch_geometric.datasets import ZINC\n",
        "from torch_geometric.loader import DataLoader\n",
        "from torch_geometric.nn import GINEConv, global_add_pool\n",
        "import inspect\n",
        "from typing import Any, Dict, Optional\n",
        "\n",
        "import torch.nn.functional as F\n",
        "from torch import Tensor\n",
        "from torch.nn import Dropout, Linear, Sequential\n",
        "\n",
        "from torch_geometric.nn.conv import MessagePassing\n",
        "from torch_geometric.nn.inits import reset\n",
        "from torch_geometric.nn.resolver import (\n",
        "    activation_resolver,\n",
        "    normalization_resolver,\n",
        ")\n",
        "from torch_geometric.typing import Adj\n",
        "from torch_geometric.utils import to_dense_batch\n",
        "\n",
        "from mamba_ssm import Mamba\n",
        "from torch_geometric.utils import degree, sort_edge_index\n",
        "import torch_sparse\n",
        "#import torch_geometric"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip uninstall torch-sparse torch-geometric torch-scatter torch-cluster"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hey10yLJTHkq",
        "outputId": "96a015ac-8f39-4936-fcda-678bdf961eae"
      },
      "id": "hey10yLJTHkq",
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found existing installation: torch_sparse 0.6.18\n",
            "Uninstalling torch_sparse-0.6.18:\n",
            "  Would remove:\n",
            "    /usr/local/lib/python3.10/dist-packages/torch_sparse-0.6.18.dist-info/*\n",
            "    /usr/local/lib/python3.10/dist-packages/torch_sparse/*\n",
            "Proceed (Y/n)? Y\n",
            "  Successfully uninstalled torch_sparse-0.6.18\n",
            "Found existing installation: torch_geometric 2.3.1\n",
            "Uninstalling torch_geometric-2.3.1:\n",
            "  Would remove:\n",
            "    /usr/local/lib/python3.10/dist-packages/torch_geometric-2.3.1.dist-info/*\n",
            "    /usr/local/lib/python3.10/dist-packages/torch_geometric/*\n",
            "Proceed (Y/n)? Y\n",
            "  Successfully uninstalled torch_geometric-2.3.1\n",
            "Found existing installation: torch_scatter 2.1.2\n",
            "Uninstalling torch_scatter-2.1.2:\n",
            "  Would remove:\n",
            "    /usr/local/lib/python3.10/dist-packages/torch_scatter-2.1.2.dist-info/*\n",
            "    /usr/local/lib/python3.10/dist-packages/torch_scatter/*\n",
            "Proceed (Y/n)? Y\n",
            "  Successfully uninstalled torch_scatter-2.1.2\n",
            "\u001b[33mWARNING: Skipping torch-cluster as it is not installed.\u001b[0m\u001b[33m\n",
            "\u001b[0m"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "print(torch.__version__)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6FXU_bOvTPOC",
        "outputId": "1c78e7c2-47a4-4622-ec76-c9de7677fafc"
      },
      "id": "6FXU_bOvTPOC",
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2.4.1+cu121\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install torch-sparse torch-scatter torch-cluster torch-spline-conv torch-geometric -f https://data.pyg.org/whl/torch-2.4.1+cu121.html"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 984
        },
        "id": "Ie0V8LZtTZLq",
        "outputId": "27d7c87f-0202-4656-9aab-f2d0269fa9e5"
      },
      "id": "Ie0V8LZtTZLq",
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in links: https://data.pyg.org/whl/torch-2.4.1+cu121.html\n",
            "Collecting torch-sparse\n",
            "  Using cached torch_sparse-0.6.18-cp310-cp310-linux_x86_64.whl\n",
            "Collecting torch-scatter\n",
            "  Using cached torch_scatter-2.1.2-cp310-cp310-linux_x86_64.whl\n",
            "Collecting torch-cluster\n",
            "  Using cached torch_cluster-1.6.3.tar.gz (54 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting torch-spline-conv\n",
            "  Using cached torch_spline_conv-1.2.2.tar.gz (25 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting torch-geometric\n",
            "  Downloading torch_geometric-2.6.1-py3-none-any.whl.metadata (63 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m63.1/63.1 kB\u001b[0m \u001b[31m4.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: scipy in /usr/local/lib/python3.10/dist-packages (from torch-sparse) (1.13.1)\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.10/dist-packages (from torch-geometric) (3.10.10)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch-geometric) (2024.6.1)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch-geometric) (3.1.4)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from torch-geometric) (1.26.4)\n",
            "Requirement already satisfied: psutil>=5.8.0 in /usr/local/lib/python3.10/dist-packages (from torch-geometric) (5.9.5)\n",
            "Requirement already satisfied: pyparsing in /usr/local/lib/python3.10/dist-packages (from torch-geometric) (3.1.4)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from torch-geometric) (2.32.3)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from torch-geometric) (4.66.5)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->torch-geometric) (2.4.3)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp->torch-geometric) (1.3.1)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->torch-geometric) (24.2.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp->torch-geometric) (1.4.1)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp->torch-geometric) (6.1.0)\n",
            "Requirement already satisfied: yarl<2.0,>=1.12.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->torch-geometric) (1.14.0)\n",
            "Requirement already satisfied: async-timeout<5.0,>=4.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->torch-geometric) (4.0.3)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch-geometric) (3.0.1)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->torch-geometric) (3.4.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->torch-geometric) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->torch-geometric) (2.2.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->torch-geometric) (2024.8.30)\n",
            "Requirement already satisfied: typing-extensions>=4.1.0 in /usr/local/lib/python3.10/dist-packages (from multidict<7.0,>=4.5->aiohttp->torch-geometric) (4.12.2)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.10/dist-packages (from yarl<2.0,>=1.12.0->aiohttp->torch-geometric) (0.2.0)\n",
            "Downloading torch_geometric-2.6.1-py3-none-any.whl (1.1 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.1/1.1 MB\u001b[0m \u001b[31m43.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hBuilding wheels for collected packages: torch-cluster, torch-spline-conv\n",
            "  Building wheel for torch-cluster (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for torch-cluster: filename=torch_cluster-1.6.3-cp310-cp310-linux_x86_64.whl size=731660 sha256=9428de47be15966f2647f55cf254fd881335f7544aa403da229cb5294f525af8\n",
            "  Stored in directory: /root/.cache/pip/wheels/51/78/c3/536637b3cdcc3313aa5e8851a6c72b97f6a01877e68c7595e3\n",
            "  Building wheel for torch-spline-conv (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for torch-spline-conv: filename=torch_spline_conv-1.2.2-cp310-cp310-linux_x86_64.whl size=225391 sha256=83b09eb4c7ec360c35e4c60d1e82b8705428499ae2115f911dfbe1c086af05b4\n",
            "  Stored in directory: /root/.cache/pip/wheels/fc/34/be/187e4b5f5ccefecca2c1a5dfc8da244ec50baa1f33c7b8c9a1\n",
            "Successfully built torch-cluster torch-spline-conv\n",
            "Installing collected packages: torch-spline-conv, torch-scatter, torch-sparse, torch-cluster, torch-geometric\n",
            "Successfully installed torch-cluster-1.6.3 torch-geometric-2.6.1 torch-scatter-2.1.2 torch-sparse-0.6.18 torch-spline-conv-1.2.2\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "torch_scatter",
                  "torch_sparse"
                ]
              },
              "id": "23e4888e2528454f8844ca88f021a5b2"
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "id": "1131a620-e047-4191-a843-f7f285877e89",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1131a620-e047-4191-a843-f7f285877e89",
        "outputId": "7cf619c7-ed0f-4fe1-a9fc-a337498ed0d1"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Processing...\n",
            "Processing train dataset: 100%|██████████| 10000/10000 [00:08<00:00, 1180.83it/s]\n",
            "Processing val dataset: 100%|██████████| 1000/1000 [00:01<00:00, 669.96it/s]\n",
            "Processing test dataset: 100%|██████████| 1000/1000 [00:00<00:00, 1107.82it/s]\n",
            "Done!\n"
          ]
        }
      ],
      "source": [
        "path, subset = '/scratch/ssd004/scratch/tsepaole/ZINC_full/', False\n",
        "path, subset = '', True\n",
        "\n",
        "transform = T.AddRandomWalkPE(walk_length=20, attr_name='pe')\n",
        "train_dataset = ZINC(path, subset=subset, split='train', pre_transform=transform)\n",
        "val_dataset = ZINC(path, subset=subset, split='val', pre_transform=transform)\n",
        "test_dataset = ZINC(path, subset=subset, split='test', pre_transform=transform)\n",
        "\n",
        "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
        "val_loader = DataLoader(val_dataset, batch_size=64)\n",
        "test_loader = DataLoader(test_dataset, batch_size=64)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "id": "34dc6712-8c4a-44f7-94e9-d582c5e16bcd",
      "metadata": {
        "id": "34dc6712-8c4a-44f7-94e9-d582c5e16bcd"
      },
      "outputs": [],
      "source": [
        "class GPSConv(torch.nn.Module):\n",
        "\n",
        "    def __init__(\n",
        "        self,\n",
        "        channels: int,\n",
        "        conv: Optional[MessagePassing],\n",
        "        heads: int = 1,\n",
        "        dropout: float = 0.0,\n",
        "        attn_dropout: float = 0.0,\n",
        "        act: str = 'relu',\n",
        "        att_type: str = 'transformer',\n",
        "        order_by_degree: bool = False,\n",
        "        shuffle_ind: int = 0,\n",
        "        d_state: int = 16,\n",
        "        d_conv: int = 4,\n",
        "        act_kwargs: Optional[Dict[str, Any]] = None,\n",
        "        norm: Optional[str] = 'batch_norm',\n",
        "        norm_kwargs: Optional[Dict[str, Any]] = None,\n",
        "    ):\n",
        "        super().__init__()\n",
        "\n",
        "        self.channels = channels\n",
        "        self.conv = conv\n",
        "        self.heads = heads\n",
        "        self.dropout = dropout\n",
        "        self.att_type = att_type\n",
        "        self.shuffle_ind = shuffle_ind\n",
        "        self.order_by_degree = order_by_degree\n",
        "\n",
        "        assert (self.order_by_degree==True and self.shuffle_ind==0) or (self.order_by_degree==False), f'order_by_degree={self.order_by_degree} and shuffle_ind={self.shuffle_ind}'\n",
        "        # order_by_degree flag is set to True, this section reorders the nodes based on their degrees\n",
        "        if self.att_type == 'transformer':\n",
        "            self.attn = torch.nn.MultiheadAttention(\n",
        "                channels,\n",
        "                heads,\n",
        "                dropout=attn_dropout,\n",
        "                batch_first=True,\n",
        "            )\n",
        "        if self.att_type == 'mamba':\n",
        "            self.self_attn = Mamba(\n",
        "                d_model=channels,\n",
        "                d_state=d_state,\n",
        "                d_conv=d_conv,\n",
        "                expand=1\n",
        "            )\n",
        "\n",
        "        self.mlp = Sequential(\n",
        "            Linear(channels, channels * 2),\n",
        "            activation_resolver(act, **(act_kwargs or {})),\n",
        "            Dropout(dropout),\n",
        "            Linear(channels * 2, channels),\n",
        "            Dropout(dropout),\n",
        "        )\n",
        "\n",
        "        norm_kwargs = norm_kwargs or {}\n",
        "        self.norm1 = normalization_resolver(norm, channels, **norm_kwargs)\n",
        "        self.norm2 = normalization_resolver(norm, channels, **norm_kwargs)\n",
        "        self.norm3 = normalization_resolver(norm, channels, **norm_kwargs)\n",
        "\n",
        "        self.norm_with_batch = False\n",
        "        if self.norm1 is not None:\n",
        "            signature = inspect.signature(self.norm1.forward)\n",
        "            self.norm_with_batch = 'batch' in signature.parameters\n",
        "\n",
        "    def reset_parameters(self):\n",
        "        r\"\"\"Resets all learnable parameters of the module.\"\"\"\n",
        "        if self.conv is not None:\n",
        "            self.conv.reset_parameters()\n",
        "        self.attn._reset_parameters()\n",
        "        reset(self.mlp)\n",
        "        if self.norm1 is not None:\n",
        "            self.norm1.reset_parameters()\n",
        "        if self.norm2 is not None:\n",
        "            self.norm2.reset_parameters()\n",
        "        if self.norm3 is not None:\n",
        "            self.norm3.reset_parameters()\n",
        "\n",
        "    def forward(\n",
        "        self,\n",
        "        x: Tensor,\n",
        "        edge_index: Adj,\n",
        "        batch: Optional[torch.Tensor] = None,\n",
        "        **kwargs,\n",
        "    ) -> Tensor:\n",
        "        r\"\"\"Runs the forward pass of the module.\"\"\"\n",
        "        hs = []\n",
        "        if self.conv is not None:  # Local MPNN.\n",
        "            h = self.conv(x, edge_index, **kwargs)\n",
        "            h = F.dropout(h, p=self.dropout, training=self.training)\n",
        "            h = h + x\n",
        "            if self.norm1 is not None:\n",
        "                if self.norm_with_batch:\n",
        "                    h = self.norm1(h, batch=batch)\n",
        "                else:\n",
        "                    h = self.norm1(h)\n",
        "            hs.append(h)\n",
        "\n",
        "        ### Global attention transformer-style model.\n",
        "        if self.att_type == 'transformer':\n",
        "            h, mask = to_dense_batch(x, batch)\n",
        "            h, _ = self.attn(h, h, h, key_padding_mask=~mask, need_weights=False)\n",
        "            h = h[mask]\n",
        "\n",
        "        if self.att_type == 'mamba':\n",
        "\n",
        "            if self.order_by_degree:\n",
        "              # degree() function: edge_index[0]= source nodes of the edges, x.shape[0]= number of nodes)\n",
        "                deg = degree(edge_index[0], x.shape[0]).to(torch.long)\n",
        "                order_tensor = torch.stack([batch, deg], 1).T\n",
        "                # creates a tensor by stacking the 'batch' tensor(batch_index of each node) and the degree of each node. making it easier to reorder nodes\n",
        "                _, x = sort_edge_index(order_tensor, edge_attr=x)\n",
        "                # reorders the feature tensor x based on the node degree and batch information, reordering x as well.\n",
        "\n",
        "            if self.shuffle_ind == 0:\n",
        "              # no shuffling occurs, and dense batch repr. is created from x\n",
        "                h, mask = to_dense_batch(x, batch)\n",
        "                h = self.self_attn(h)[mask]\n",
        "                # the result is masked to remove invalid entries (padded entries)\n",
        "                #\n",
        "            else:\n",
        "                mamba_arr = []\n",
        "                # list to store the results of multiple attention passes over shuffled node features\n",
        "                for _ in range(self.shuffle_ind):\n",
        "                  # loops over the number of shuffles(shuffle_ind)\n",
        "                    h_ind_perm = permute_within_batch(x, batch)\n",
        "                    # Similar to before, converts the shuffled x into a dense representation for each bat\n",
        "                    h_i, mask = to_dense_batch(x[h_ind_perm], batch)\n",
        "                    # Similar to before, converts the shuffled x into a dense representation for each batch.\n",
        "                    h_i = self.self_attn(h_i)[mask][h_ind_perm]\n",
        "                    # The shuffled dense batch is passed through the self-attention mechanism, and the result is reordered back to the original node order (h_ind_perm).\n",
        "                    mamba_arr.append(h_i)\n",
        "                h = sum(mamba_arr) / self.shuffle_ind\n",
        "\n",
        "                # Averages the results from all shuffled versions to produce the final node representations.\n",
        "        ###\n",
        "\n",
        "        h = F.dropout(h, p=self.dropout, training=self.training)\n",
        "        h = h + x  # Residual connection.\n",
        "        if self.norm2 is not None:\n",
        "            if self.norm_with_batch:\n",
        "                h = self.norm2(h, batch=batch)\n",
        "            else:\n",
        "                h = self.norm2(h)\n",
        "        hs.append(h)\n",
        "\n",
        "        out = sum(hs)  # Combine local and global outputs.\n",
        "\n",
        "        out = out + self.mlp(out)\n",
        "        if self.norm3 is not None:\n",
        "            if self.norm_with_batch:\n",
        "                out = self.norm3(out, batch=batch)\n",
        "            else:\n",
        "                out = self.norm3(out)\n",
        "\n",
        "        return out\n",
        "\n",
        "    def __repr__(self) -> str:\n",
        "        return (f'{self.__class__.__name__}({self.channels}, '\n",
        "                f'conv={self.conv}, heads={self.heads})')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "id": "d9fea5c7-9fc9-4388-98fd-8c45787d0abc",
      "metadata": {
        "id": "d9fea5c7-9fc9-4388-98fd-8c45787d0abc"
      },
      "outputs": [],
      "source": [
        "class GraphModel(torch.nn.Module):\n",
        "    def __init__(self, channels: int, pe_dim: int, num_layers: int, model_type: str, shuffle_ind: int, d_state: int, d_conv: int, order_by_degree: False):\n",
        "        super().__init__()\n",
        "\n",
        "        # pe_dim: Dimensionality of the positional encoding\n",
        "        # num_layers = ?\n",
        "        # shuffle_ind: number of shuffle indices for the Mamba attention mechanism\n",
        "        # d_state, d_conv: parameters of Mamba attention\n",
        "        # order_by_degree: If nodes should be ordered by degree\n",
        "\n",
        "        self.node_emb = Embedding(28, channels - pe_dim)\n",
        "        # An embedding layer that maps node input features into the space of channels - pe_dim dimensions.\n",
        "        self.pe_lin = Linear(20, pe_dim)\n",
        "        # A linear layer that transforms the positional encoding (pe) from 20 dimensions to pe_dim.\n",
        "        self.pe_norm = BatchNorm1d(20)\n",
        "        # Batch normalization applied to the positional encoding to normalize it.\n",
        "        self.edge_emb = Embedding(4, channels)\n",
        "        # Embedding for edge attributes, mapping them from 4 features into the same space as the nodes (channels).\n",
        "        self.model_type = model_type\n",
        "        self.shuffle_ind = shuffle_ind\n",
        "        self.order_by_degree = order_by_degree\n",
        "\n",
        "        # Graph Convolution layers\n",
        "        self.convs = ModuleList()\n",
        "        # A list of graph convolutional layers\n",
        "\n",
        "        # Creates a two-layer neural network (nn), used within each GINEConv layer\n",
        "        for _ in range(num_layers):\n",
        "            nn = Sequential(\n",
        "                Linear(channels, channels),\n",
        "                ReLU(),\n",
        "                Linear(channels, channels),\n",
        "            )\n",
        "            if self.model_type == 'gine':\n",
        "                conv = GINEConv(nn)\n",
        "\n",
        "            if self.model_type == 'mamba':\n",
        "                conv = GPSConv(channels, GINEConv(nn), heads=4, attn_dropout=0.5,\n",
        "                               att_type='mamba',\n",
        "                               shuffle_ind=self.shuffle_ind,\n",
        "                               order_by_degree=self.order_by_degree,\n",
        "                               d_state=d_state, d_conv=d_conv)\n",
        "\n",
        "            if self.model_type == 'transformer':\n",
        "                conv = GPSConv(channels, GINEConv(nn), heads=4, attn_dropout=0.5, att_type='transformer')\n",
        "\n",
        "            # conv = GINEConv(nn)\n",
        "            self.convs.append(conv)\n",
        "\n",
        "        self.mlp = Sequential(\n",
        "            Linear(channels, channels // 2),\n",
        "            ReLU(),\n",
        "            Linear(channels // 2, channels // 4),\n",
        "            ReLU(),\n",
        "            Linear(channels // 4, 1),\n",
        "        )\n",
        "\n",
        "    def forward(self, x, pe, edge_index, edge_attr, batch):\n",
        "      # x: Node attributes\n",
        "      # pe: Positional encoding for nodes\n",
        "      # edge_index: edge indices of the graph\n",
        "      # edge_attr: edge attributes\n",
        "      # batch\n",
        "        x_pe = self.pe_norm(pe)\n",
        "        # Batch normalization to positional encodings\n",
        "\n",
        "        x = torch.cat((self.node_emb(x.squeeze(-1)), self.pe_lin(x_pe)), 1)\n",
        "        # embedding applied to the node features\n",
        "        # linear transformation applied to x_pe\n",
        "        # concatenate along feature dimension\n",
        "\n",
        "        edge_attr = self.edge_emb(edge_attr)\n",
        "        # edge embedding applied to edge attributes, transforms into the same dim. as nodes\n",
        "\n",
        "        for conv in self.convs:\n",
        "            if self.model_type == 'gine':\n",
        "                x = conv(x, edge_index, edge_attr=edge_attr)\n",
        "            else:\n",
        "                x = conv(x, edge_index, batch, edge_attr=edge_attr)\n",
        "\n",
        "        x = global_add_pool(x, batch)\n",
        "        # node features are pooled at the graph level using global pooling\n",
        "        return self.mlp(x)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "85c6d04a-4eff-45ef-be39-93e4b520a967",
      "metadata": {
        "id": "85c6d04a-4eff-45ef-be39-93e4b520a967"
      },
      "outputs": [],
      "source": [
        "def train():\n",
        "    model.train()\n",
        "\n",
        "    total_loss = 0\n",
        "    for data in train_loader:\n",
        "        data = data.to(device)\n",
        "        optimizer.zero_grad()\n",
        "        out = model(data.x, data.pe, data.edge_index, data.edge_attr,\n",
        "                    data.batch)\n",
        "        loss = (out.squeeze() - data.y).abs().mean()\n",
        "        loss.backward()\n",
        "        total_loss += loss.item() * data.num_graphs\n",
        "        optimizer.step()\n",
        "    return total_loss / len(train_loader.dataset)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "0ddc967c-1c69-4d26-aa30-20c212c92c81",
      "metadata": {
        "id": "0ddc967c-1c69-4d26-aa30-20c212c92c81"
      },
      "outputs": [],
      "source": [
        "@torch.no_grad()\n",
        "def test(loader):\n",
        "    model.eval()\n",
        "\n",
        "    total_error = 0\n",
        "    for data in loader:\n",
        "        data = data.to(device)\n",
        "        # print(data.x.shape)\n",
        "        out = model(data.x, data.pe, data.edge_index, data.edge_attr,\n",
        "                    data.batch)\n",
        "        total_error += (out.squeeze() - data.y).abs().sum().item()\n",
        "    return total_error / len(loader.dataset)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "03d5de1c-0c2d-4fd4-81db-58f5f4257764",
      "metadata": {
        "id": "03d5de1c-0c2d-4fd4-81db-58f5f4257764"
      },
      "outputs": [],
      "source": [
        "# it.to(device)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "cf9b7ab1-54e5-415d-be4a-c43742c33608",
      "metadata": {
        "tags": [],
        "id": "cf9b7ab1-54e5-415d-be4a-c43742c33608"
      },
      "outputs": [],
      "source": [
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "model = GraphModel(channels=64, pe_dim=8, num_layers=10,\n",
        "                   model_type='mamba',\n",
        "                   shuffle_ind=0, order_by_degree=True,\n",
        "                   d_conv=4, d_state=16,\n",
        "                  ).to(device)\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=0.001, weight_decay=1e-5)\n",
        "scheduler = ReduceLROnPlateau(optimizer, mode='min', factor=0.5, patience=20,\n",
        "                              min_lr=0.00001)\n",
        "arr = []\n",
        "for epoch in range(1, 30):\n",
        "    loss = train()\n",
        "    val_mae = test(val_loader)\n",
        "    test_mae = test(test_loader)\n",
        "    scheduler.step(val_mae)\n",
        "    print(f'Epoch: {epoch:02d}, Loss: {loss:.4f}, Val: {val_mae:.4f}, '\n",
        "          f'Test: {test_mae:.4f}')\n",
        "    arr.append(test_mae)\n",
        "ordering = arr\n",
        "print(ordering)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "dadbc115-6b16-4266-af79-75ec97b304a6",
      "metadata": {
        "id": "dadbc115-6b16-4266-af79-75ec97b304a6"
      },
      "outputs": [],
      "source": [
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "model = GraphModel(channels=64, pe_dim=8, num_layers=10,\n",
        "                   model_type='mamba',\n",
        "                   shuffle_ind=1, order_by_degree=False,\n",
        "                   d_conv=4, d_state=16,\n",
        "                  ).to(device)\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=0.001, weight_decay=1e-5)\n",
        "scheduler = ReduceLROnPlateau(optimizer, mode='min', factor=0.5, patience=20,\n",
        "                              min_lr=0.00001)\n",
        "arr = []\n",
        "for epoch in range(1, 30):\n",
        "    loss = train()\n",
        "    val_mae = test(val_loader)\n",
        "    test_mae = test(test_loader)\n",
        "    scheduler.step(val_mae)\n",
        "    print(f'Epoch: {epoch:02d}, Loss: {loss:.4f}, Val: {val_mae:.4f}, '\n",
        "          f'Test: {test_mae:.4f}')\n",
        "    arr.append(test_mae)\n",
        "permute = arr\n",
        "print(permute)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "15ab378c-1563-4177-8078-61b036aa9c6c",
      "metadata": {
        "id": "15ab378c-1563-4177-8078-61b036aa9c6c"
      },
      "outputs": [],
      "source": [
        "# import matplotlib.pyplot as plt\n",
        "# import pandas as pd\n",
        "\n",
        "# import numpy as np\n",
        "# res_df = pd.read_csv('30_ep_res.csv')\n",
        "\n",
        "# WINDOW = 1\n",
        "# fig, ax = plt.subplots(1, figsize=(15,5))\n",
        "\n",
        "# for col in res_df.columns:\n",
        "#     plt.plot(res_df[col].clip(0,0.7).rolling(WINDOW, min_periods=1).mean(), label=col)\n",
        "\n",
        "# plt.legend()\n",
        "# plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "15426d6e-d6a4-4b46-80c8-dae456caaa4f",
      "metadata": {
        "id": "15426d6e-d6a4-4b46-80c8-dae456caaa4f"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "fig, ax = plt.subplots(1, figsize=(15,5))\n",
        "\n",
        "plt.plot(permute[:20], label='permute')\n",
        "plt.plot(ordering[:20], label='order')\n",
        "\n",
        "plt.legend()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "67d97587-f99a-46b9-9f7b-74d81d42fca0",
      "metadata": {
        "id": "67d97587-f99a-46b9-9f7b-74d81d42fca0"
      },
      "outputs": [],
      "source": [
        "fffffffffff"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a0c5b0c6-64be-4a4b-9e3f-62a76355dce4",
      "metadata": {
        "id": "a0c5b0c6-64be-4a4b-9e3f-62a76355dce4"
      },
      "source": [
        "# Plotting"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "4c05860c-e4cc-4e49-ad4c-e746a22ea8e0",
      "metadata": {
        "id": "4c05860c-e4cc-4e49-ad4c-e746a22ea8e0"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "id": "41bf4f39-8190-4b24-b391-b31b161eca7b",
      "metadata": {
        "id": "41bf4f39-8190-4b24-b391-b31b161eca7b"
      },
      "source": [
        "# Tests"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "0bc68822-3415-4685-9e62-371dc919bd93",
      "metadata": {
        "id": "0bc68822-3415-4685-9e62-371dc919bd93"
      },
      "outputs": [],
      "source": [
        "ggggggggggg"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "47643725-13bc-4e29-9bc5-0d748a10d951",
      "metadata": {
        "id": "47643725-13bc-4e29-9bc5-0d748a10d951"
      },
      "outputs": [],
      "source": [
        "it = next(iter(train_loader))\n",
        "# h, mask = to_dense_batch(it.x, it.batch)\n",
        "# it.x.shape, h.shape, mask.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "1955eeda-6fc6-4f4f-8c97-b0643a9c3b6b",
      "metadata": {
        "id": "1955eeda-6fc6-4f4f-8c97-b0643a9c3b6b"
      },
      "outputs": [],
      "source": [
        "deg.dtype"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d359e9b1-e3ba-4ff8-8ff6-3a17e7deb306",
      "metadata": {
        "id": "d359e9b1-e3ba-4ff8-8ff6-3a17e7deb306"
      },
      "outputs": [],
      "source": [
        "it"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "5f7447ec-f9ab-43de-8a39-f2707f4fb020",
      "metadata": {
        "id": "5f7447ec-f9ab-43de-8a39-f2707f4fb020"
      },
      "outputs": [],
      "source": [
        "it.edge_index[0]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e281930c-5708-4cf9-866f-c1d2efde207c",
      "metadata": {
        "id": "e281930c-5708-4cf9-866f-c1d2efde207c"
      },
      "outputs": [],
      "source": [
        "it.to(device)\n",
        "out = model(it.x, it.pe, it.edge_index, it.edge_attr,\n",
        "                    it.batch)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "85e47e6b-2721-4eeb-9c70-c8fc269cc4a9",
      "metadata": {
        "id": "85e47e6b-2721-4eeb-9c70-c8fc269cc4a9"
      },
      "outputs": [],
      "source": [
        "batch = torch.tensor([0,0,0,1,1,1,1])\n",
        "x = torch.tensor([0,1,2,3,4,5,6])\n",
        "batch.shape, x.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ef569b66-a435-4f2a-96ad-f33177ca9435",
      "metadata": {
        "id": "ef569b66-a435-4f2a-96ad-f33177ca9435"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "\n",
        "def permute_within_batch(x, batch):\n",
        "    # Enumerate over unique batch indices\n",
        "    unique_batches = torch.unique(batch)\n",
        "\n",
        "    # Initialize list to store permuted indices\n",
        "    permuted_indices = []\n",
        "\n",
        "    for batch_index in unique_batches:\n",
        "        # Extract indices for the current batch\n",
        "        indices_in_batch = (batch == batch_index).nonzero().squeeze()\n",
        "\n",
        "        # Permute indices within the current batch\n",
        "        permuted_indices_in_batch = indices_in_batch[torch.randperm(len(indices_in_batch))]\n",
        "\n",
        "        # Append permuted indices to the list\n",
        "        permuted_indices.append(permuted_indices_in_batch)\n",
        "\n",
        "    # Concatenate permuted indices into a single tensor\n",
        "    permuted_indices = torch.cat(permuted_indices)\n",
        "\n",
        "    return permuted_indices\n",
        "\n",
        "# Example usage\n",
        "batch = torch.tensor([0, 0, 0, 1, 1, 1, 1])\n",
        "x = torch.tensor([0, 10, 20, 30, 40, 50, 60])\n",
        "\n",
        "# Get permuted indices\n",
        "permuted_indices = permute_within_batch(x, batch)\n",
        "\n",
        "# Use permuted indices to get the permuted tensor\n",
        "permuted_x = x[permuted_indices]\n",
        "\n",
        "print(\"Original x:\", x)\n",
        "print(\"Permuted x:\", permuted_x)\n",
        "print(\"Permuted indices:\", permuted_indices)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "eca35cc4-faaa-417d-abb0-8da383ecef2c",
      "metadata": {
        "id": "eca35cc4-faaa-417d-abb0-8da383ecef2c"
      },
      "outputs": [],
      "source": [
        "mask[0].sum(), (it.batch==0).sum()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "5e624feb-987b-4cc7-b7a0-388d2b6f121e",
      "metadata": {
        "id": "5e624feb-987b-4cc7-b7a0-388d2b6f121e"
      },
      "outputs": [],
      "source": [
        "self_attn = Mamba(d_model=64, # Model dimension d_model\n",
        "                                d_state=16,  # SSM state expansion factor\n",
        "                                d_conv=4,    # Local convolution width\n",
        "                                expand=1,    # Block expansion factor\n",
        "                            )\n",
        "print(sum(p.numel() for p in self_attn.parameters() if p.requires_grad), sum(p.numel() for p in self_attn.parameters()))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "61d5c45a-6d97-4659-99d5-14c63cec9818",
      "metadata": {
        "id": "61d5c45a-6d97-4659-99d5-14c63cec9818"
      },
      "outputs": [],
      "source": [
        "self_attn = Mamba(d_model=64, # Model dimension d_model\n",
        "                                d_state=8,  # SSM state expansion factor\n",
        "                                d_conv=2,    # Local convolution width\n",
        "                                expand=1,    # Block expansion factor\n",
        "                            )\n",
        "print(sum(p.numel() for p in self_attn.parameters() if p.requires_grad), sum(p.numel() for p in self_attn.parameters()))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f1359039-8fa5-4f01-a4c6-2171dc188300",
      "metadata": {
        "id": "f1359039-8fa5-4f01-a4c6-2171dc188300"
      },
      "outputs": [],
      "source": [
        "self_attn = Mamba(d_model=64, # Model dimension d_model\n",
        "                                d_state=16,  # SSM state expansion factor\n",
        "                                d_conv=8,    # Local convolution width\n",
        "                                expand=1,    # Block expansion factor\n",
        "                            )\n",
        "print(sum(p.numel() for p in self_attn.parameters() if p.requires_grad), sum(p.numel() for p in self_attn.parameters()))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "0bd4f3f9-2eef-4731-8ae0-a00e7fdaa6b6",
      "metadata": {
        "id": "0bd4f3f9-2eef-4731-8ae0-a00e7fdaa6b6"
      },
      "outputs": [],
      "source": [
        "self_attn = torch.nn.MultiheadAttention(\n",
        "                64,\n",
        "                4,\n",
        "                dropout=0.5,\n",
        "                batch_first=True,\n",
        "            )\n",
        "print(sum(p.numel() for p in self_attn.parameters() if p.requires_grad), sum(p.numel() for p in self_attn.parameters()))"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "mols",
      "language": "python",
      "name": "mols"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.16"
    },
    "colab": {
      "provenance": [],
      "machine_shape": "hm"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
